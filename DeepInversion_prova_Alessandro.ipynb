{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepInversion_prova_Alessandro.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOFtbRJAs/vC11W6bXQ4cTJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6231bc717dd5453785f99a33b5dda114": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5b78f85b903a4e5f8b4fc19632a633e1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d56cb565f93e4280a06cd0af9cd3af9b",
              "IPY_MODEL_b9fc6f5a2b0941c8a251c623ddc65ec7"
            ]
          }
        },
        "5b78f85b903a4e5f8b4fc19632a633e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d56cb565f93e4280a06cd0af9cd3af9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7091c37439f34b18b295c75c0f91c4bf",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 100,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 100,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a271acccf6674cc892a9c1b6da6b0c5e"
          }
        },
        "b9fc6f5a2b0941c8a251c623ddc65ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5f710ed00c0e4f969f1e8c582e5dab68",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 100/100 [55:03&lt;00:00, 33.03s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_11e465749718416c88f608299920f38b"
          }
        },
        "7091c37439f34b18b295c75c0f91c4bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a271acccf6674cc892a9c1b6da6b0c5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f710ed00c0e4f969f1e8c582e5dab68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "11e465749718416c88f608299920f38b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8144ac86d5c44d8fa4eaac462a9dfa42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6528317c3c474513b49c0c8936c564d5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fb1cfb103f5d4ddea923785d2df5c976",
              "IPY_MODEL_79f71bc6fc6346eb8333ab4acbefac00"
            ]
          }
        },
        "6528317c3c474513b49c0c8936c564d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb1cfb103f5d4ddea923785d2df5c976": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_00e4d39278244ef79422af219a367494",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c58899c0a1f9482382eeac46b4456603"
          }
        },
        "79f71bc6fc6346eb8333ab4acbefac00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bb1701dedc8c452b889028491781f4e3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/70 [48:57&lt;00:00, 41.97s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d12aa3338fc743a58cf6771ad89ba144"
          }
        },
        "00e4d39278244ef79422af219a367494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c58899c0a1f9482382eeac46b4456603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb1701dedc8c452b889028491781f4e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d12aa3338fc743a58cf6771ad89ba144": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5bc7f6ccb80a43eabaf1bdf02e24ece2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d89e4a4b204047d1926bf269fdee14f0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_959b6c00d6094263827c1776d7d8da5e",
              "IPY_MODEL_8ef6b65348484c4c8da084f2557a4f4e"
            ]
          }
        },
        "d89e4a4b204047d1926bf269fdee14f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "959b6c00d6094263827c1776d7d8da5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5fd36a917203484f8ed6f297ca86d726",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1500,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1500,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_779960904ed446c7b06a430f38b2f5f5"
          }
        },
        "8ef6b65348484c4c8da084f2557a4f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_92848c6c36844570b6657ffd6a305a8f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1500/1500 [02:58&lt;00:00,  8.40it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2e897941ec694ee1a1d8b87fa7b5c77f"
          }
        },
        "5fd36a917203484f8ed6f297ca86d726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "779960904ed446c7b06a430f38b2f5f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92848c6c36844570b6657ffd6a305a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2e897941ec694ee1a1d8b87fa7b5c77f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlePa98/DeepInversion/blob/master/DeepInversion_prova_Alessandro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfqenFowb1xL",
        "outputId": "f3e0e3d0-817f-4bc3-c1f2-da613fbcc8d8"
      },
      "source": [
        "!pip3 install 'import_ipynb'\n",
        "!pip3 install 'tqdm'\n",
        " \n",
        "!rm -r IncrementalLearning\n",
        "# upload work files from your git hub repository\n",
        "import sys\n",
        " \n",
        "!git clone https://github.com/alessandronicolini/IncrementalLearning.git # clone proj repository\n",
        "!rm -rf IncrementalLearning/README.md \n",
        "!rm -rf IncrementalLearning/baselines.ipynb\n",
        " \n",
        "path = 'IncrementalLearning/'\n",
        "if path not in sys.path:\n",
        "    sys.path.append('IncrementalLearning/')\n",
        " \n",
        "!pip3 install import_ipynb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import_ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=3f957495c759d07f9d5d8a8ca8c17f4f5408aa655744dee3c62f9bca2f72665d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "rm: cannot remove 'IncrementalLearning': No such file or directory\n",
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 166, done.\u001b[K\n",
            "remote: Counting objects: 100% (166/166), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 640 (delta 102), reused 0 (delta 0), pack-reused 474\u001b[K\n",
            "Receiving objects: 100% (640/640), 860.31 KiB | 8.35 MiB/s, done.\n",
            "Resolving deltas: 100% (377/377), done.\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us1alRIbb5pW"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "import random\n",
        "import torchvision\n",
        "ROOT = './data'\n",
        "class ilCIFAR100(VisionDataset):\n",
        "    \"\"\"\n",
        "    Extends CIFAR100 class. Split the dataset into 10 batches, each one containing 10 classes.\n",
        "    You can retrieve the batches from the attribute \"batches\", it has different structure according to\n",
        "    test and train CIFAR100 splits:\n",
        "        - train -> batches is a dictionary {0:{'train':indexes, 'val':indexes}...} \n",
        "        - test -> batches is a dictionary {0:indexes...}\n",
        "    where the keys are the batch number.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory\n",
        "            `cifar-10-batches-py` exists or will be saved to if download is set to True.\n",
        "        seed(int): used to ensure reproducibility in shuffling operations.\n",
        "        val_size(float, optional): between 0 and 1, fraction of data used for validation.\n",
        "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
        "            creates from test set.\n",
        "        transform (callable, optional): A function/transform that takes in an PIL image\n",
        "            and returns a transformed version. E.g, `transforms.RandomCrop`\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "    \"\"\"\n",
        "    def __init__(self,classes_per_batch, seed, val_size=0.1, train=True, transform=None, target_transform=False, \n",
        "    download=True):\n",
        "        \n",
        "        super(ilCIFAR100, self).__init__(root=0)\n",
        "        self.classes_per_batch=classes_per_batch\n",
        "\n",
        "        \n",
        "        self.__rs = seed # set random seed \n",
        "        self.train=train\n",
        "        self.__transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "\n",
        "        self.__transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "        # if train:\n",
        "        #     self.batches = self.__make_train_batches(val_size)\n",
        "        # else:\n",
        "        #     self.batches = self.__make_test_batches()\n",
        "        if self.train == 'train':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self.__transform_train)\n",
        "        elif self.train == 'exemplars':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self.__transform_test)\n",
        "        else:\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=False,\n",
        "                                        download=True, transform=self.__transform_test)\n",
        "          \n",
        "        self.targets = np.array(self.dataset.targets) # make targets an array to exploit masking\n",
        "        random.seed(seed)\n",
        "        self.classes = random.sample(range(0, 100), 100)\n",
        "        #self.classes = self.classes.reshape((10, -1)) # each row contains the classes for the corrisponding batch\n",
        "        #print(self.classes)\n",
        "        self.__dictionary = {}\n",
        "        for i, c in enumerate(self.classes):\n",
        "          self.__dictionary[c] = i\n",
        "\n",
        "\n",
        "    def get_dict(self):\n",
        "      return self.__dictionary\n",
        "    def __getitem__(self, index):\n",
        "        return index,self.dataset.__getitem__(index)[0],self.dataset.__getitem__(index)[1]\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__()\n",
        "    def getbatches(self):\n",
        "      classlist=self.classes\n",
        "      batches=[]\n",
        "      for i in range(0,int(100/self.classes_per_batch)):\n",
        "        #print(i)\n",
        "        batch=classlist[int(i*self.classes_per_batch):int(i*10+self.classes_per_batch)]\n",
        "        batches.append(batch)\n",
        "      return batches\n",
        "    def get_batch_indexes(self):\n",
        "      classlist=self.classes\n",
        "      numclass=self.classes_per_batch\n",
        "      batch_indexes=[]\n",
        "      for i in range(0,int(100/self.classes_per_batch)):\n",
        "        batch=classlist[int(i*numclass):int(i*numclass+numclass)]\n",
        "        mask=np.isin(self.targets,batch)\n",
        "        indexes=np.array(np.arange(len(self.dataset.targets)))\n",
        "        indexes=indexes[mask]\n",
        "        batch_indexes.append(indexes)\n",
        "      return batch_indexes\n",
        "    def get_class_indexes(self,label):\n",
        "      indexes = np.array(np.arange(len(self.dataset.targets)))\n",
        "      labels = self.dataset.targets\n",
        "      mask = np.isin(labels, label)\n",
        "      indexes = indexes[mask]\n",
        "\n",
        "      return indexes\n",
        "    def get_train_val(self,valid):\n",
        "      batches=self.get_batch_indexes()\n",
        "      train=[]\n",
        "      val=[]\n",
        "      for batch in batches:\n",
        "        #print(type(batch))\n",
        "        random.shuffle(batch)\n",
        "        valbatch=batch[0:int(valid*len(batch))]\n",
        "        trainbatch=batch[int(valid*len(batch)):]\n",
        "        train.append(trainbatch)\n",
        "        val.append(valbatch)\n",
        "      return train,val\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import time\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    #CIFAR100\n",
        "    mean = [0.5071, 0.4867, 0.4408] \n",
        "    std = [0.2675, 0.2565, 0.2761]\n",
        "    #CIFAR10\n",
        "    #mean = [0.4914, 0.4822, 0.4465]\n",
        "    #std = [0.2023, 0.1994, 0.2010]\n",
        "    if not isinstance(input_image, np.ndarray):\n",
        "        if isinstance(input_image, torch.Tensor):\n",
        "            image_tensor = input_image.data\n",
        "        else:\n",
        "            return input_image\n",
        "        image_numpy = image_tensor.cpu().detach().float().numpy()\n",
        "        if image_numpy.shape[0] == 1:\n",
        "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "        for i in range(len(mean)): \n",
        "            image_numpy[i] = image_numpy[i] * std[i] + mean[i]\n",
        "        image_numpy = image_numpy * 255\n",
        "        image_numpy = np.transpose(image_numpy, (1, 2, 0))\n",
        "    else:\n",
        "        image_numpy = input_image\n",
        "    return image_numpy.astype(imtype)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTxZa5_vb7x-",
        "outputId": "d6ab4a90-3cdf-4c17-8d97-2f6a03d21250"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "from sklearn.preprocessing import normalize\n",
        "import copy\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from torch.utils.data import Subset, DataLoader, Dataset\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "import import_ipynb\n",
        "#from IncrementalLearning.cifar100 import ilCIFAR100\n",
        "\n",
        "from IncrementalLearning.resnet_cifar import resnet32\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from /content/IncrementalLearning/resnet_cifar.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "E_tW2Uqnb_wT",
        "outputId": "4707a711-3c31-4993-f62e-538a3e1b3fce"
      },
      "source": [
        "class mnemonics():\n",
        "  def __init__(self, randomseed):\n",
        "    self.device = 'cuda'\n",
        "    self.model = resnet32(num_classes=100).to(self.device)\n",
        "    self.feature_extractor = self.model.features\n",
        "    self.temp_model = None\n",
        "    self.lr = 2\n",
        "    self.gamma = 0.2\n",
        "    self.weight_decay = 1e-5 \n",
        "    self.milestones = [49,63,93]\n",
        "    self.batch_size = 128\n",
        "    self.numepochs = 100\n",
        "    self.n_classes = 0\n",
        "    self.n_known = 0\n",
        "    self.feature_size=64\n",
        "    self.momentum=0.9\n",
        "    self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    self.NUM_BATCHES=10\n",
        "    self.randomseed=randomseed\n",
        "    self.trainloader=None\n",
        "    self.testloader=None\n",
        "    self.CLASSES_PER_BATCH=10\n",
        "\n",
        "    self.original_training_set = ilCIFAR100(self.CLASSES_PER_BATCH, self.randomseed, train = 'train')\n",
        "    self.original_exemplar_set = ilCIFAR100(self.CLASSES_PER_BATCH, self.randomseed, train = 'exemplars')\n",
        "    self.original_test_set = ilCIFAR100(self.CLASSES_PER_BATCH,self.randomseed, train= 'test')\n",
        "\n",
        "    self.last_test = None\n",
        "    self.y_pred = []\n",
        "    self.y_test = []\n",
        "\n",
        "    self.cumulative_class_mean = []\n",
        "\n",
        "    self.classes_seen=0\n",
        "    self.diz = self.original_training_set.get_dict()\n",
        "\n",
        "    self.exemplar_features_mean = None\n",
        "    # lista di liste, ogni lista contiene gli exemplars di una classe\n",
        "    self.exemplar_sets_idxs = [] # mn_exemplat_sets\n",
        "    # lista unica, tutti gli indici degli exemplar\n",
        "    self.exemplar_idxs = []\n",
        "\n",
        "  '''\n",
        "  def update_params(self, \n",
        "                    m,\n",
        "                    finetuning_idxs, \n",
        "                    training_idxs, \n",
        "                    mnemonics_to_optimize, \n",
        "                    batch_size,\n",
        "                    new=True,\n",
        "                    lr=10, \n",
        "                    momentum=0.9, \n",
        "                    weight_decay=1e-5, \n",
        "                    milestones=[10, 20, 30, 40],\n",
        "                    gamma=0.5, \n",
        "                    tuning_epochs=4,\n",
        "                    updating_epochs=50):\n",
        "    \n",
        "    \"\"\"\n",
        "    finetuning_idxs = indexes of current task elements\n",
        "    mnemonics_idxs = indexes of exemplar elements\n",
        "    mnemonics_to_optimize = the optimized parameters in the update phase\n",
        "    \"\"\"\n",
        "\n",
        "    # make a copy of the model\n",
        "    model_copy = copy.deepcopy(self.model)\n",
        "    model_copy.train()\n",
        "    model_copy.to(self.device)\n",
        "\n",
        "    # define the loss\n",
        "    # criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # FINE TUNING FOR 1 EPOCH eq. 8 --------------------------------------------\n",
        "    \n",
        "    # define optimizer and scheduler for fine tuning phase\n",
        "    optimizer = optim.SGD(model_copy.parameters(), lr=2, momentum=momentum, weight_decay=weight_decay)\n",
        "    \n",
        "    # create the subset dataset to load the data you want, and the loader\n",
        "    finetuning_labels = np.array([self.original_training_set.__getitem__(idx)[2] for idx in finetuning_idxs], dtype=int)\n",
        "    meta_idxs = [i for i in range(len(finetuning_idxs))]\n",
        "    random.shuffle(meta_idxs)\n",
        "\n",
        "    # split the meta idxs in batches\n",
        "    n_batches = int(np.floor(len(finetuning_idxs)/batch_size))\n",
        "    meta_idxs_batches = []\n",
        "    for i in range(n_batches):\n",
        "      meta_idxs_batches.append(np.array(meta_idxs[batch_size*i:batch_size*(i+1)]))\n",
        "    meta_idxs_batches.append(np.array(meta_idxs[batch_size*n_batches:]))\n",
        "\n",
        "    # now fine tune the copied model\n",
        "    for epoch in range(tuning_epochs):\n",
        "      for meta_idxs_batch in meta_idxs_batches:\n",
        "        inputs = mnemonics_to_optimize[0][meta_idxs_batch] # are already in cuda\n",
        "        labels = finetuning_labels[meta_idxs_batch]\n",
        "        labels = torch.tensor([self.diz[c] for c in labels])\n",
        "        labels = labels.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_copy(inputs)\n",
        "        labels_encoded = F.one_hot(labels,100).float().to(self.device)\n",
        "        loss = self.criterion(outputs, labels_encoded)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "\n",
        "    # UPDATE THE MNEMONICS eq.9/10 ---------------------------------------------\n",
        "    \n",
        "    model_copy.eval()\n",
        "    \n",
        "    optimizer = optim.SGD(mnemonics_to_optimize, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "    \n",
        "\n",
        "    if new:\n",
        "      exlvl_training = Subset(self.original_training_set, training_idxs)\n",
        "      exlvl_loader = DataLoader(exlvl_training, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "      current_task_labels = set([self.original_training_set.__getitem__(idx)[2] for idx in training_idxs])\n",
        "      new_dict = {label:new_label for label, new_label in zip(current_task_labels, range(10))}\n",
        "\n",
        "      new_class_mean = {new_dict[key] : value for key, value in self.cumulative_class_mean.items()}\n",
        "      means_ready = torch.Tensor(list(new_class_mean.values())).to(self.device)\n",
        "\n",
        "    \n",
        "    else:\n",
        "      exlvl_training = Subset(self.original_exemplar_set, training_idxs)\n",
        "      exlvl_loader = DataLoader(exlvl_training, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    print('lunghezza del exemplar update:', len(training_idxs))\n",
        "    for epoch in tqdm(range(updating_epochs)):\n",
        "\n",
        "      for _, inputs, labels in exlvl_loader:\n",
        "\n",
        "        if new:\n",
        "          labels = torch.tensor([new_dict[c.item()] for c in labels])\n",
        "        else:\n",
        "          labels = torch.tensor([self.diz[c.item()] for c in labels])\n",
        "\n",
        "        labels = labels.to(self.device)\n",
        "        inputs = inputs.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        out_features = model_copy.features(inputs)\n",
        "        # compute features mean of mnemonics for each class\n",
        "        if new:\n",
        "          the_logits = F.linear(F.normalize(out_features, p=2, dim=1), F.normalize(means_ready, p=2, dim=1))\n",
        "        else:\n",
        "          ##Da capire questa cosa!!!!!!! se provassi a mettere self?\n",
        "          #OPPURE uso un altro tensore copiando all_class_means_calcolato_fuori?  \n",
        "          #dov'Ã¨ il gradiente?\n",
        "          n_classes = int(len(finetuning_labels)/m)\n",
        "          all_class_means = torch.zeros((0, 64))\n",
        "          all_class_means = all_class_means.to(self.device)\n",
        "          for i in range(n_classes): # how many classes\n",
        "            mnemonics_features = model_copy.features(mnemonics_to_optimize[0][i*m:(i+1)*m])\n",
        "            this_class_means = torch.mean(mnemonics_features, dim=0) # size 64\n",
        "            this_class_means = torch.unsqueeze(this_class_means, dim=0) # add the second dimension\n",
        "            all_class_means = torch.cat((all_class_means, this_class_means), dim=0)\n",
        "          the_logits = F.linear(F.normalize(out_features, p=2, dim=1), F.normalize(all_class_means, p=2, dim=1))\n",
        "\n",
        "        #labels_encoded = F.one_hot(labels,100).float().cuda()\n",
        "\n",
        "        loss = F.cross_entropy(the_logits, labels) # al secondo batch di classi per i new mnemonics le uscite sono sempre 10 ma le label vanno da 10 a 19\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "  def exemplar_level_optimization(self, m, task_num, current_task_indices):  \n",
        "    \n",
        "    # UPDATING NEW EXEMPLAR-----------------------------------------------------\n",
        "\n",
        "    # isola gli indici dei nuovi exemplars\n",
        "    new_exemplar_idxs = []\n",
        "    for idxs in self.exemplar_sets_idxs[-10:]:\n",
        "      new_exemplar_idxs += idxs\n",
        "\n",
        "    # ora ottieni gli mnemonics che poi sono da ottimizzare\n",
        "    new_mnemonics_data = torch.zeros((10*m, 3, 32, 32))\n",
        "\n",
        "    for i, idx in enumerate(new_exemplar_idxs):\n",
        "      new_mnemonics_data[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "\n",
        "    new_mnemonics = nn.ParameterList()\n",
        "    new_mnemonics.append(nn.Parameter(new_mnemonics_data))\n",
        "    new_mnemonics.to(self.device)\n",
        "    \n",
        "    #print(new_mnemonics[0][0])\n",
        "\n",
        "    options_new ={'finetuning_idxs': new_exemplar_idxs, \n",
        "                  'training_idxs': current_task_indices, \n",
        "                  'mnemonics_to_optimize':  new_mnemonics,  \n",
        "                  'batch_size':128,\n",
        "                  'm':m}\n",
        "\n",
        "    print('---start mnemonics updating---')\n",
        "\n",
        "    self.update_params(**options_new)    \n",
        "\n",
        "    for i, idx in enumerate(new_exemplar_idxs):\n",
        "      self.original_exemplar_set.dataset.data[idx] = tensor2im(new_mnemonics[0][i])\n",
        "\n",
        "    \n",
        "    # UPDATING OLD EXEMPLARS ---------------------------------------------------\n",
        "\n",
        "    if task_num:\n",
        "      # decidi quanti elementi ha ogni exemlar set in a e in b a seconda se m Ã¨ \n",
        "      # pari o dispari\n",
        "      if m%2:\n",
        "        l_a = int((m+1)/2)\n",
        "      else:\n",
        "        l_a = int(m/2)\n",
        "      l_b = int(m-l_a)\n",
        "\n",
        "      # isola gli indici dei vecchi exemplars, dividendoli in due parti\n",
        "      # ogni classe deve avere circa la metÃ  degli exemplar originali\n",
        "      old_exemplar_idxs_a = []\n",
        "      old_exemplar_idxs_b = []\n",
        "      \n",
        "      for idxs in self.exemplar_sets_idxs[:-10]:\n",
        "        old_exemplar_idxs_a += idxs[:l_a]\n",
        "        old_exemplar_idxs_b += idxs[l_a:]\n",
        "\n",
        "      old_mnemonics_data_a = torch.zeros((task_num*10*l_a, 3, 32, 32))\n",
        "      old_mnemonics_data_b = torch.zeros((task_num*10*l_b, 3, 32, 32))\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_a):\n",
        "        old_mnemonics_data_a[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "      \n",
        "      for i, idx in enumerate(old_exemplar_idxs_b):\n",
        "          old_mnemonics_data_b[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "      \n",
        "      old_mnemonics_a = nn.ParameterList()\n",
        "      old_mnemonics_a.append(nn.Parameter(old_mnemonics_data_a))\n",
        "      old_mnemonics_a.to(self.device)\n",
        "      old_mnemonics_b = nn.ParameterList()\n",
        "      old_mnemonics_b.append(nn.Parameter(old_mnemonics_data_b))\n",
        "      old_mnemonics_b.to(self.device)\n",
        "\n",
        "      options_old_a = {'finetuning_idxs':old_exemplar_idxs_a, \n",
        "                       'training_idxs':old_exemplar_idxs_b, \n",
        "                       'mnemonics_to_optimize':old_mnemonics_a, \n",
        "                       'batch_size':128,\n",
        "                       'm': l_a,\n",
        "                       'new':False}\n",
        "\n",
        "      options_old_b = {'finetuning_idxs':old_exemplar_idxs_b, \n",
        "                       'training_idxs':old_exemplar_idxs_a, \n",
        "                       'mnemonics_to_optimize':old_mnemonics_b, \n",
        "                       'batch_size':128,\n",
        "                       'm':l_b,\n",
        "                       'new':False}\n",
        "\n",
        "      self.update_params(**options_old_a) \n",
        "      self.update_params(**options_old_b)\n",
        "\n",
        "      # CONVERT AND STORE UPDATED EXEMPLAR as numpy array\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_a):\n",
        "        self.original_exemplar_set.dataset.data[idx] = tensor2im(old_mnemonics_a[0][i])\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_b):\n",
        "        self.original_exemplar_set.dataset.data[idx] = tensor2im(old_mnemonics_b[0][i])\n",
        "    \n",
        "\n",
        "\n",
        "    # FINE TUNE THE CURRENT NET ON ALL THE EXEMPLARS COLLECTED 'TILL NOW\n",
        "   \n",
        "  '''\n",
        "\n",
        "  def model_level_optimization(self):\n",
        "    \n",
        "    old_model = copy.deepcopy(self.model)\n",
        "    old_model.eval()\n",
        "    old_model.to(self.device)\n",
        "    n_classes = self.classes_seen+self.CLASSES_PER_BATCH\n",
        "    print(n_classes)\n",
        "    optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.milestones, gamma=self.gamma)\n",
        "    for epoch in tqdm(range(self.numepochs)):\n",
        "        \n",
        "      for _, inputs, labels in self.trainloader:\n",
        "        inputs = inputs.float().to(self.device)\n",
        "        labels = torch.tensor([self.diz[c.item()] for c in labels])\n",
        "\n",
        "        labels=labels.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs=self.model(inputs)\n",
        "\n",
        "        labels_encoded = F.one_hot(labels,100).float().to(self.device) #CAMBIARE ONE_HOT\n",
        "        \n",
        "        if self.classes_seen:\n",
        "          old_target = old_model(inputs).to(self.device)\n",
        "          old_target = torch.sigmoid(old_target).to(self.device)\n",
        "          \n",
        "          target = torch.cat((old_target[:,:self.classes_seen], labels_encoded[:, self.classes_seen:]), dim=1)\n",
        "          loss = self.criterion(outputs, target)\n",
        "        else:\n",
        "          loss = self.criterion(outputs, labels_encoded) \n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      \n",
        "      scheduler.step()\n",
        "\n",
        "\n",
        "  def classify_nme(self, input_batch):\n",
        "    min_distances = float('inf')*torch.ones(len(input_batch)).to(self.device) # shape: batch_size --> 128\n",
        "    y_pred = torch.zeros(len(input_batch), dtype=torch.int8).to(self.device) # shape: batch_size --> 128\n",
        "    input_features = self.model.features(input_batch) # shape: (batch_size, feature_size) --> (128, 64)\n",
        "\n",
        "    for i in range(len(self.exemplar_sets_idxs)):\n",
        "      ex_mean = self.exemplar_means[i,:]\n",
        "\n",
        "      # compute distances between inputs features and exemplar set means\n",
        "      pdist = nn.PairwiseDistance(p=2)\n",
        "      distances = pdist(input_features, ex_mean) # shape: batch_size --> 128\n",
        "\n",
        "      # update min distancies and predicted labels\n",
        "      mask = distances < min_distances\n",
        "      min_distances[mask] = distances[mask]\n",
        "      y_pred[mask] = self.exemplar_labels[i]\n",
        "\n",
        "    return y_pred\n",
        "    \n",
        "\n",
        "\n",
        "  def get_new_exemplars(self, batch, m):\n",
        "    loader = torch.utils.data.DataLoader(batch, batch_size=self.batch_size,shuffle=False, num_workers=4)\n",
        "    features = np.zeros((0,self.feature_size))\n",
        "    indices = np.zeros((0), dtype=int)\n",
        "    with torch.no_grad():\n",
        "      for indexes, images, labels in loader:\n",
        "        images = images.cuda()\n",
        "        feature = self.feature_extractor(images).data.cpu().numpy()\n",
        "        feature = normalize(feature, axis=1, norm='l2')\n",
        "        features = np.concatenate((features,feature), axis=0)\n",
        "        indices = np.concatenate((indices,indexes), axis=0)\n",
        "\n",
        "    class_mean = np.mean(features, axis=0)\n",
        "    class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n",
        "\n",
        "    #self.cumulative_class_mean.append(class_mean)\n",
        "\n",
        "    exemplar_set = []\n",
        "    exemplar_features = np.zeros((0,self.feature_size))\n",
        "\n",
        "    for k in range(1, int(m)+1):\n",
        "        S = np.sum(exemplar_features, axis=0)\n",
        "        phi = features\n",
        "        mu = class_mean\n",
        "        mu_p = 1.0 / k * (phi + S)\n",
        "        mu_p = normalize(mu_p, axis=1, norm='l2')\n",
        "        i = np.argmin(np.sqrt(np.sum((mu - mu_p) ** 2, axis=1)))\n",
        "        exemplar_set.append(int(indices[i]))\n",
        "        addfeature =  np.expand_dims(features[i], axis=0)\n",
        "        exemplar_features = np.concatenate((exemplar_features,addfeature), axis=0)\n",
        "\n",
        "        #remove duplicates\n",
        "        features = np.delete(features, i, 0)\n",
        "        indices = np.delete(indices, i, 0)\n",
        "        \n",
        "    self.exemplar_sets_idxs.append(exemplar_set)\n",
        "    #self.exemplar_sets_idxs.append(random.sample(list(batch), m))\n",
        "\n",
        "\n",
        "  def reduce_old_exemplars(self, m):\n",
        "    for i, set_i in enumerate(self.exemplar_sets_idxs):\n",
        "      self.exemplar_sets_idxs[i] = random.sample(set_i, m)\n",
        "\n",
        "  def __accuracy_fc(self, dl, mapper):\n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    for  _, images, labels in dl:\n",
        "      labels = torch.tensor([torch.tensor(mapper[c.item()]) for c in labels])\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      outputs = self.model(images)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def __accuracy_nme(self, dl):\n",
        "    \n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    \n",
        "    for  _, images, labels in dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      preds = self.classify_nme(images)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "      if self.last_test:\n",
        "        self.y_pred += preds.tolist()\n",
        "        self.y_test += labels.tolist()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def plot_confusion_matrix(self):\n",
        " \n",
        "    cm = confusion_matrix(self.y_test, self.y_pred)\n",
        "    cm = np.log(cm+1)\n",
        "    fig, ax = plt.subplots(figsize=(7,7))\n",
        "    sns.heatmap(cm, square=True, cbar=False, ax=ax, cmap=plt.get_cmap('seismic'))\n",
        "    ax.set_xticks(np.linspace(19,99,5))\n",
        "    ax.set_yticks(np.linspace(19,99,5))\n",
        "    ax.set_xticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_yticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_title(\"iCaRL\")\n",
        "    ax.set_xlabel(\"Predicted class\")\n",
        "    ax.set_ylabel(\"True class\")\n",
        "    plt.savefig(\"iCaRL_\"+str(self.randomseed)+\"_cm.png\")\n",
        "    plt.show()\n",
        "    return cm\n",
        "\n",
        "  def plot_data(self, train_dl):\n",
        "\n",
        "    from sklearn.manifold import TSNE\n",
        "    print('------plot data------')\n",
        "\n",
        "    #Data points\n",
        "    train_labels_array = torch.zeros(0).to('cuda')\n",
        "    train_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in train_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      train_dataset_to_reduce = np.concatenate((train_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      train_labels_array = torch.cat((train_labels_array, labels))\n",
        "\n",
        "    \n",
        "    #EX e MN loaders \n",
        "    current_exemplar_indices = np.array([], dtype=int)\n",
        "\n",
        "    for exemplar_set in self.exemplar_sets_idxs:\n",
        "      current_exemplar_indices = np.concatenate([current_exemplar_indices, np.array(exemplar_set)])\n",
        "\n",
        "    exemplar_dataset = Subset(self.original_training_set, current_exemplar_indices)\n",
        "    ex_dl = DataLoader(exemplar_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) #usato solo per plottare\n",
        "\n",
        "    mn_dataset = Subset(self.original_exemplar_set, current_exemplar_indices)\n",
        "    mn_dl = DataLoader(mn_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) #usato solo per plottare\n",
        "\n",
        "\n",
        "    #Exemplars\n",
        "\n",
        "    ex_labels_array = torch.zeros(0).to('cuda')\n",
        "    ex_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in ex_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      ex_dataset_to_reduce = np.concatenate((ex_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      ex_labels_array = torch.cat((ex_labels_array, labels), dim = 0)\n",
        "\n",
        "\n",
        "    #Mnemonics\n",
        "\n",
        "\n",
        "    mn_labels_array = torch.zeros(0).to('cuda')\n",
        "    mn_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in mn_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      mn_dataset_to_reduce = np.concatenate((mn_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      mn_labels_array = torch.cat((mn_labels_array, labels), dim = 0)\n",
        "\n",
        "    #PLOT'''\n",
        "    total_data_w_exemplars = np.concatenate((train_dataset_to_reduce, ex_dataset_to_reduce))\n",
        "    total_data_w_mn =  np.concatenate((train_dataset_to_reduce, mn_dataset_to_reduce))\n",
        "\n",
        "    total_transformed_ex = TSNE(n_components=2).fit_transform(total_data_w_exemplars)\n",
        "    X_transformed_w_ex = total_transformed_ex[:train_dataset_to_reduce.shape[0]]\n",
        "    ex_transformed = total_transformed_ex[train_dataset_to_reduce.shape[0]:]\n",
        "\n",
        "    total_transformed_mn = TSNE(n_components=2).fit_transform(total_data_w_mn)\n",
        "    X_transformed_w_mn = total_transformed_mn[:train_dataset_to_reduce.shape[0]]\n",
        "    mn_transformed = total_transformed_mn[train_dataset_to_reduce.shape[0]:]\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(24,12))\n",
        "    ax1.scatter(X_transformed_w_ex[:,0], X_transformed_w_ex[:,1], c = train_labels_array.cpu(), alpha = 0.1)\n",
        "    ax1.scatter(ex_transformed[:,0], ex_transformed[:,1], c = ex_labels_array.cpu(), alpha = 1)\n",
        "    #ax1.title('EXEMPLARS')\n",
        "\n",
        "    ax2.scatter(X_transformed_w_mn[:,0], X_transformed_w_mn[:,1], c = train_labels_array.cpu(), alpha = 0.1)\n",
        "    ax2.scatter(mn_transformed[:,0], mn_transformed[:,1], c = mn_labels_array.cpu(), alpha = 1)\n",
        "    #ax2.title('MNEMONICS')\n",
        "    plt.show()\n",
        "\n",
        "  def trainer(self):\n",
        "    \n",
        "    train_indices = self.original_training_set.get_batch_indexes()\n",
        "    test_indices = self.original_test_set.get_batch_indexes()\n",
        "    batches=self.original_training_set.getbatches()\n",
        "    current_test_indexes=[]\n",
        "    test_acc = []\n",
        "    self.last_test = False\n",
        "\n",
        "    for i in range(self.NUM_BATCHES):\n",
        "      print('current batches', batches[i])\n",
        "      if i == self.NUM_BATCHES-1:\n",
        "        self.last_test = True\n",
        "\n",
        "      current_exemplar_indices = np.array([], dtype=int)\n",
        "    \n",
        "      for exemplar_set in self.exemplar_sets_idxs:\n",
        "        current_exemplar_indices = np.concatenate([current_exemplar_indices, np.array(exemplar_set)])\n",
        "\n",
        "      exemplar_dataset = Subset(self.original_training_set, current_exemplar_indices) \n",
        "      #DA CAMBIARE CON SELF.ORIGINAL EXEMPLAR SET\n",
        "      if i > 1: #FINETUNING\n",
        "        print('----inizio finetuning----')\n",
        "        print('numbero of classes in the exemplar sets', len(self.exemplar_sets_idxs))\n",
        "        self.numepochs = 10\n",
        "        self.lr = 0.2\n",
        "        temporary_classes_seen = self.classes_seen\n",
        "        self.classes_seen = 0\n",
        "        self.trainloader = DataLoader(exemplar_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) \n",
        "        print('accuracy on exemplar set before finetuining:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "        self.model.train()\n",
        "        self.model_level_optimization()\n",
        "        #BACK TO THE NORMAL PARAMETERS\n",
        "        self.model.eval()\n",
        "        self.numepochs = 70\n",
        "        self.lr = 2\n",
        "        self.classes_seen = temporary_classes_seen\n",
        "        print('accuracy on exemplar set after finetuining:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "        current_test_acc = self.__accuracy_nme(self.testloader)\n",
        "        print('accuracy on test set after finetuning:', 100*current_test_acc)\n",
        "        print('-----fine finetuning------')\n",
        "        print('-'*80)\n",
        "\n",
        "      train_dataset = Subset(self.original_training_set, train_indices[i])\n",
        "      current_test_indexes += test_indices[i].tolist()\n",
        "      test_dataset = Subset(self.original_test_set,current_test_indexes)\n",
        "      self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "      self.testloader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True)        \n",
        "      \n",
        "\n",
        "      if i == 0:\n",
        "        self.trainloader = self.train_loader\n",
        "      else:\n",
        "        self.trainloader = DataLoader(torch.utils.data.ConcatDataset([train_dataset, exemplar_dataset]), batch_size=self.batch_size, shuffle=True,\n",
        "          num_workers=4, pin_memory=True)\n",
        "\n",
        "        \n",
        "      self.model.train()\n",
        "      self.model_level_optimization()    \n",
        "      self.classes_seen += 10\n",
        "      self.model.eval() # Set Network to evaluation mode\n",
        "      m=int(2000/(int(i*10+10)))\n",
        "      return self.model, batches[i]\n",
        "      \n",
        "      break\n",
        "'''\n",
        "      #NUOVO PAPER DEL PORCODDIO\n",
        "      labels_of_modified = torch.zeros(0, dtype = int).to('cuda')\n",
        "      for label in batches[i]:\n",
        "        labels = torch.LongTensor([self.diz[label]]*m).to('cuda')\n",
        "        labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\n",
        "      print('labels to be created', labels_of_modified)\n",
        "      print('len to be created', len(labels_of_modified))\n",
        "      number_of_images_created = m*10\n",
        "      net_student = resnet32(num_classes=100).to(self.device)\n",
        "      data_type = torch.float\n",
        "      inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device='cuda', dtype=data_type)\n",
        "\n",
        "      net_student = copy.deepcopy(self.model)\n",
        "      net_student.eval() #important, otherwise generated images will be non natural\n",
        "      \n",
        "      train_writer = None  # tensorboard writter\n",
        "      global_iteration = 0\n",
        "      di_lr = 0.05\n",
        "      optimizer_di = optim.Adam([inputs], lr=di_lr)\n",
        "\n",
        "      print(\"Starting model inversion\")\n",
        "      batch_idx = 0\n",
        "      inputs = get_images(net=self.model, bs=200, epochs=2000, idx=batch_idx,\n",
        "                        net_student=net_student,\n",
        "                        train_writer=train_writer, use_amp=False,\n",
        "                        optimizer=optimizer_di, inputs=inputs, \n",
        "                        var_scale=0.00005, labels=labels)\n",
        "\n",
        "\n",
        "      plt.imshow(tensor2im(inputs[0]))\n",
        "      plt.show()\n",
        "      plt.imshow(tensor2im(inputs[55]))\n",
        "      plt.show()\n",
        "      print('deepinversion finshed')\n",
        "      # update exemplars number\n",
        "      \n",
        "\n",
        "      # reduce the number of each exemplars set\n",
        "      self.reduce_old_exemplars(m) \n",
        "\n",
        "      self.cumulative_class_mean = {}\n",
        "\n",
        "      for classlabel in batches[i]:\n",
        "        indexes_class = self.original_training_set.get_class_indexes(classlabel)\n",
        "        current_class = Subset(self.original_training_set, indexes_class)\n",
        "        #self.get_new_exemplars(indexes_class, m)\n",
        "        self.get_new_exemplars(current_class, m)\n",
        "        indexes_class = self.original_training_set.get_class_indexes(classlabel)\n",
        "        current_class = Subset(self.original_training_set, indexes_class)\n",
        "\n",
        "        loader = torch.utils.data.DataLoader(current_class, batch_size=self.batch_size,shuffle=False, num_workers=4)\n",
        "        features = np.zeros((0, 64))\n",
        "        with torch.no_grad():\n",
        "          for indexes, images, labels in loader:\n",
        "            images = images.cuda()\n",
        "            feature = self.feature_extractor(images).data.cpu().numpy()\n",
        "            feature = normalize(feature, axis=1, norm='l2')\n",
        "            features = np.concatenate((features,feature), axis=0)\n",
        "\n",
        "        class_mean = np.mean(features, axis=0)\n",
        "        class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n",
        "\n",
        "        self.cumulative_class_mean[classlabel] = class_mean\n",
        "        \n",
        "      #plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      #plt.show()\n",
        "      \n",
        "  \n",
        "      #self.exemplar_level_optimization(m, i, self.original_training_set.get_batch_indexes()[i])\n",
        "\n",
        "      # compute means of exemplar set\n",
        "      # cycle for each exemplar set\n",
        "      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device)\n",
        "      self.exemplar_labels = []\n",
        "      for j in range(len(self.exemplar_sets_idxs)):\n",
        "        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets_idxs[j])\n",
        "        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
        "        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device) # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\n",
        "      \n",
        "        with torch.no_grad():\n",
        "          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets_idxs[j][0]) \n",
        "          self.exemplar_labels.append(exemplar_label)\n",
        "          # cycle for each batch in the current exemplar set\n",
        "          for _,  exemplars, _ in exemplars_loader:\n",
        "          \n",
        "            # get exemplars features\n",
        "            exemplars = exemplars.to(self.device)\n",
        "            features = self.model.features(exemplars) # shape: (len(exemplars), feature_size)\n",
        "          \n",
        "            # normalize \n",
        "            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\n",
        "            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\n",
        "            features = features/feature_norms\n",
        "          \n",
        "            # concatenate over columns\n",
        "            ex_features = torch.cat((ex_features, features), dim=0)\n",
        "          \n",
        "        # compute current exemplar set mean and normalize it\n",
        "        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\n",
        "        ex_mean = ex_mean/torch.norm(ex_mean)\n",
        "        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\n",
        "        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar sets, feature size)\n",
        "      \n",
        "      #if i == 0:\n",
        "       # self.plot_data(self.trainloader)\n",
        "      print('accuracy on training set:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "      # print('accuracy on test set:', self.__accuracy_on(self.testloader,self,self.diz))\n",
        "      current_test_acc = self.__accuracy_nme(self.testloader)\n",
        "      print('accuracy on test set:', 100*current_test_acc)\n",
        "      print('-' * 80)\n",
        "      test_acc.append(current_test_acc)\n",
        "\n",
        "    # compute comfusion matrix and save results\n",
        "    cm = self.plot_confusion_matrix()\n",
        "    with open('iCaRL_'+str(self.randomseed)+\"_cm\", 'wb') as file:\n",
        "      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open('iCaRL_'+str(self.randomseed)+\"_testacc\", 'wb') as file:\n",
        "      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "      print('PRINT IMAGES')\n",
        "      print('with data augmentation')\n",
        "      plt.imshow(self.original_training_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      plt.show()\n",
        "      print('without data augmentation')\n",
        "      plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      plt.show()\n",
        "'''"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n      #NUOVO PAPER DEL PORCODDIO\\n      labels_of_modified = torch.zeros(0, dtype = int).to(\\'cuda\\')\\n      for label in batches[i]:\\n        labels = torch.LongTensor([self.diz[label]]*m).to(\\'cuda\\')\\n        labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\\n      print(\\'labels to be created\\', labels_of_modified)\\n      print(\\'len to be created\\', len(labels_of_modified))\\n      number_of_images_created = m*10\\n      net_student = resnet32(num_classes=100).to(self.device)\\n      data_type = torch.float\\n      inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device=\\'cuda\\', dtype=data_type)\\n\\n      net_student = copy.deepcopy(self.model)\\n      net_student.eval() #important, otherwise generated images will be non natural\\n      \\n      train_writer = None  # tensorboard writter\\n      global_iteration = 0\\n      di_lr = 0.05\\n      optimizer_di = optim.Adam([inputs], lr=di_lr)\\n\\n      print(\"Starting model inversion\")\\n      batch_idx = 0\\n      inputs = get_images(net=self.model, bs=200, epochs=2000, idx=batch_idx,\\n                        net_student=net_student,\\n                        train_writer=train_writer, use_amp=False,\\n                        optimizer=optimizer_di, inputs=inputs, \\n                        var_scale=0.00005, labels=labels)\\n\\n\\n      plt.imshow(tensor2im(inputs[0]))\\n      plt.show()\\n      plt.imshow(tensor2im(inputs[55]))\\n      plt.show()\\n      print(\\'deepinversion finshed\\')\\n      # update exemplars number\\n      \\n\\n      # reduce the number of each exemplars set\\n      self.reduce_old_exemplars(m) \\n\\n      self.cumulative_class_mean = {}\\n\\n      for classlabel in batches[i]:\\n        indexes_class = self.original_training_set.get_class_indexes(classlabel)\\n        current_class = Subset(self.original_training_set, indexes_class)\\n        #self.get_new_exemplars(indexes_class, m)\\n        self.get_new_exemplars(current_class, m)\\n        indexes_class = self.original_training_set.get_class_indexes(classlabel)\\n        current_class = Subset(self.original_training_set, indexes_class)\\n\\n        loader = torch.utils.data.DataLoader(current_class, batch_size=self.batch_size,shuffle=False, num_workers=4)\\n        features = np.zeros((0, 64))\\n        with torch.no_grad():\\n          for indexes, images, labels in loader:\\n            images = images.cuda()\\n            feature = self.feature_extractor(images).data.cpu().numpy()\\n            feature = normalize(feature, axis=1, norm=\\'l2\\')\\n            features = np.concatenate((features,feature), axis=0)\\n\\n        class_mean = np.mean(features, axis=0)\\n        class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\\n\\n        self.cumulative_class_mean[classlabel] = class_mean\\n        \\n      #plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      #plt.show()\\n      \\n  \\n      #self.exemplar_level_optimization(m, i, self.original_training_set.get_batch_indexes()[i])\\n\\n      # compute means of exemplar set\\n      # cycle for each exemplar set\\n      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device)\\n      self.exemplar_labels = []\\n      for j in range(len(self.exemplar_sets_idxs)):\\n        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets_idxs[j])\\n        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\\n        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device) # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\\n      \\n        with torch.no_grad():\\n          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets_idxs[j][0]) \\n          self.exemplar_labels.append(exemplar_label)\\n          # cycle for each batch in the current exemplar set\\n          for _,  exemplars, _ in exemplars_loader:\\n          \\n            # get exemplars features\\n            exemplars = exemplars.to(self.device)\\n            features = self.model.features(exemplars) # shape: (len(exemplars), feature_size)\\n          \\n            # normalize \\n            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\\n            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\\n            features = features/feature_norms\\n          \\n            # concatenate over columns\\n            ex_features = torch.cat((ex_features, features), dim=0)\\n          \\n        # compute current exemplar set mean and normalize it\\n        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\\n        ex_mean = ex_mean/torch.norm(ex_mean)\\n        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\\n        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar sets, feature size)\\n      \\n      #if i == 0:\\n       # self.plot_data(self.trainloader)\\n      print(\\'accuracy on training set:\\', 100*self.__accuracy_fc(self.trainloader,self.diz))\\n      # print(\\'accuracy on test set:\\', self.__accuracy_on(self.testloader,self,self.diz))\\n      current_test_acc = self.__accuracy_nme(self.testloader)\\n      print(\\'accuracy on test set:\\', 100*current_test_acc)\\n      print(\\'-\\' * 80)\\n      test_acc.append(current_test_acc)\\n\\n    # compute comfusion matrix and save results\\n    cm = self.plot_confusion_matrix()\\n    with open(\\'iCaRL_\\'+str(self.randomseed)+\"_cm\", \\'wb\\') as file:\\n      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\\n    with open(\\'iCaRL_\\'+str(self.randomseed)+\"_testacc\", \\'wb\\') as file:\\n      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)\\n\\n      print(\\'PRINT IMAGES\\')\\n      print(\\'with data augmentation\\')\\n      plt.imshow(self.original_training_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      plt.show()\\n      print(\\'without data augmentation\\')\\n      plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      plt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtarWXiNb9nG",
        "outputId": "86fc1fab-e3aa-4b05-d771-4ced838d11fa"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import argparse\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "# import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import collections\n",
        "\n",
        "#from resnet_cifar import ResNet34, ResNet18\n",
        "\n",
        "try:\n",
        "    from apex.parallel import DistributedDataParallel as DDP\n",
        "    from apex import amp, optimizers\n",
        "    USE_APEX = True\n",
        "except ImportError:\n",
        "    print(\"Please install apex from https://www.github.com/nvidia/apex to run this example.\")\n",
        "    print(\"will attempt to run without it\")\n",
        "    USE_APEX = False\n",
        "\n",
        "#provide intermeiate information\n",
        "debug_output = False\n",
        "debug_output = True\n",
        "\n",
        "\n",
        "class DeepInversionFeatureHook():\n",
        "    '''\n",
        "    Implementation of the forward hook to track feature statistics and compute a loss on them.\n",
        "    Will compute mean and variance, and will use l2 as a loss\n",
        "    '''\n",
        "\n",
        "    def __init__(self, module):\n",
        "        self.hook = module.register_forward_hook(self.hook_fn)\n",
        "\n",
        "    def hook_fn(self, module, input, output):\n",
        "        # hook co compute deepinversion's feature distribution regularization\n",
        "        nch = input[0].shape[1]\n",
        "\n",
        "        mean = input[0].mean([0, 2, 3])\n",
        "        var = input[0].permute(1, 0, 2, 3).contiguous().view([nch, -1]).var(1, unbiased=False)\n",
        "\n",
        "        # forcing mean and variance to match between two distributions\n",
        "        # other ways might work better, e.g. KL divergence\n",
        "        r_feature = torch.norm(module.running_var.data.type(var.type()) - var, 2) + torch.norm(\n",
        "            module.running_mean.data.type(var.type()) - mean, 2)\n",
        "\n",
        "        self.r_feature = r_feature\n",
        "        # must have no output\n",
        "\n",
        "    def close(self):\n",
        "        self.hook.remove()\n",
        "\n",
        "def get_images(net, bs=256, epochs=1000, idx=-1, var_scale=0.00005, competitive_scale=0.01,\n",
        "               net_student=None, prefix=None, train_writer = None, global_iteration=None,\n",
        "               use_amp=False, bn_reg_scale = 0.0,\n",
        "               optimizer = None, inputs = None, labels = False, l2_coeff=0.0):\n",
        "    '''\n",
        "    Function returns inverted images from the pretrained model, parameters are tight to CIFAR dataset\n",
        "    args in:\n",
        "        net: network to be inverted\n",
        "        bs: batch size\n",
        "        epochs: total number of iterations to generate inverted images, training longer helps a lot!\n",
        "        idx: an external flag for printing purposes: only print in the first round, set as -1 to disable\n",
        "        var_scale: the scaling factor for variance loss regularization. this may vary depending on bs\n",
        "            larger - more blurred but less noise\n",
        "        net_student: model to be used for Adaptive DeepInversion\n",
        "        prefix: defines the path to store images\n",
        "        competitive_scale: coefficient for Adaptive DeepInversion\n",
        "        train_writer: tensorboardX object to store intermediate losses\n",
        "        global_iteration: indexer to be used for tensorboard\n",
        "        use_amp: boolean to indicate usage of APEX AMP for FP16 calculations - twice faster and less memory on TensorCores\n",
        "        optimizer: potimizer to be used for model inversion\n",
        "        inputs: data place holder for optimization, will be reinitialized to noise\n",
        "        bn_reg_scale: weight for r_feature_regularization\n",
        "        random_labels: sample labels from random distribution or use columns of the same class\n",
        "        l2_coeff: coefficient for L2 loss on input\n",
        "    return:\n",
        "        A tensor on GPU with shape (bs, 3, 32, 32) for CIFAR\n",
        "    '''\n",
        "\n",
        "    kl_loss = nn.KLDivLoss(reduction='batchmean').cuda()\n",
        "\n",
        "    # preventing backpropagation through student for Adaptive DeepInversion\n",
        "    net_student.eval()\n",
        "\n",
        "    best_cost = 1e6\n",
        "\n",
        "    # initialize gaussian inputs\n",
        "    inputs.data = torch.randn((bs, 3, 32, 32), requires_grad=True, device='cuda')\n",
        "    # if use_amp:\n",
        "    #     inputs.data = inputs.data.half()\n",
        "\n",
        "    # set up criteria for optimization\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer.state = collections.defaultdict(dict)  # Reset state of optimizer\n",
        "\n",
        "    # target outputs to generate\n",
        "    #if labels:\n",
        "    targets = labels\n",
        "    #else:\n",
        "     #   targets = torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9] * 25 + [0, 1, 2, 3, 4, 5]).to('cuda')\n",
        "\n",
        "    outputs=net(inputs.data)\n",
        "    _, predicted_teach = outputs.max(1)\n",
        "\n",
        "    outputs_student=net_student(inputs.data)\n",
        "    _, predicted_std = outputs_student.max(1)\n",
        "\n",
        "    if idx == 0:\n",
        "        print('Teacher correct out of {}: {}, loss at {}'.format(bs, predicted_teach.eq(targets).sum().item(), criterion(outputs, targets).item()))\n",
        "        print('Student correct out of {}: {}, loss at {}'.format(bs, predicted_std.eq(targets).sum().item(), criterion(outputs_student, targets).item()))\n",
        "\n",
        "    ## Create hooks for feature statistics catching\n",
        "    loss_r_feature_layers = []\n",
        "    for module in net.modules():\n",
        "        if isinstance(module, nn.BatchNorm2d):\n",
        "            loss_r_feature_layers.append(DeepInversionFeatureHook(module))\n",
        "\n",
        "    # setting up the range for jitter\n",
        "    lim_0, lim_1 = 2, 2\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # apply random jitter offsets\n",
        "        off1 = random.randint(-lim_0, lim_0)\n",
        "        off2 = random.randint(-lim_1, lim_1)\n",
        "        inputs_jit = torch.roll(inputs, shifts=(off1,off2), dims=(2,3))\n",
        "\n",
        "        # foward with jit images\n",
        "        optimizer.zero_grad()\n",
        "        net.zero_grad()\n",
        "        outputs = net(inputs_jit)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss_target = loss.item()\n",
        "\n",
        "        # competition loss, Adaptive DeepInvesrion\n",
        "        if competitive_scale != 0.0:\n",
        "            net_student.zero_grad()\n",
        "            outputs_student = net_student(inputs_jit)\n",
        "            T = 3.0\n",
        "\n",
        "            if 1:\n",
        "                # jensen shanon divergence:\n",
        "                # another way to force KL between negative probabilities\n",
        "                P = F.softmax(outputs_student / T, dim=1)\n",
        "                Q = F.softmax(outputs / T, dim=1)\n",
        "                M = 0.5 * (P + Q)\n",
        "\n",
        "                P = torch.clamp(P, 0.01, 0.99)\n",
        "                Q = torch.clamp(Q, 0.01, 0.99)\n",
        "                M = torch.clamp(M, 0.01, 0.99)\n",
        "                eps = 0.0\n",
        "                # loss_verifier_cig = 0.5 * kl_loss(F.log_softmax(outputs_verifier / T, dim=1), M) +  0.5 * kl_loss(F.log_softmax(outputs/T, dim=1), M)\n",
        "                loss_verifier_cig = 0.5 * kl_loss(torch.log(P + eps), M) + 0.5 * kl_loss(torch.log(Q + eps), M)\n",
        "                # JS criteria - 0 means full correlation, 1 - means completely different\n",
        "                loss_verifier_cig = 1.0 - torch.clamp(loss_verifier_cig, 0.0, 1.0)\n",
        "\n",
        "                loss = loss + competitive_scale * loss_verifier_cig\n",
        "\n",
        "        # apply total variation regularization\n",
        "        diff1 = inputs_jit[:,:,:,:-1] - inputs_jit[:,:,:,1:]\n",
        "        diff2 = inputs_jit[:,:,:-1,:] - inputs_jit[:,:,1:,:]\n",
        "        diff3 = inputs_jit[:,:,1:,:-1] - inputs_jit[:,:,:-1,1:]\n",
        "        diff4 = inputs_jit[:,:,:-1,:-1] - inputs_jit[:,:,1:,1:]\n",
        "        loss_var = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)\n",
        "        loss = loss + var_scale*loss_var\n",
        "\n",
        "        # R_feature loss\n",
        "        loss_distr = sum([mod.r_feature for mod in loss_r_feature_layers])\n",
        "        loss = loss + bn_reg_scale*loss_distr # best for noise before BN\n",
        "\n",
        "        # l2 loss\n",
        "        if 1:\n",
        "            loss = loss + l2_coeff * torch.norm(inputs_jit, 2)\n",
        "\n",
        "        if debug_output and epoch % 200==0:\n",
        "            print(f\"It {epoch}\\t Losses: total: {loss.item():3.3f},\\ttarget: {loss_target:3.3f} \\tR_feature_loss unscaled:\\t {loss_distr.item():3.3f}\")\n",
        "            #vutils.save_image(inputs.data.clone(),\n",
        "             #                 './{}/output_{}.png'.format(prefix, epoch//200),\n",
        "              #                normalize=True, scale_each=True, nrow=10)\n",
        "\n",
        "        if best_cost > loss.item():\n",
        "            best_cost = loss.item()\n",
        "            best_inputs = inputs.data\n",
        "\n",
        "        # backward pass\n",
        "        if use_amp:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    outputs=net(best_inputs)\n",
        "    _, predicted_teach = outputs.max(1)\n",
        "\n",
        "    outputs_student=net_student(best_inputs)\n",
        "    _, predicted_std = outputs_student.max(1)\n",
        "\n",
        "    if idx == 0:\n",
        "        print('Teacher correct out of {}: {}, loss at {}'.format(bs, predicted_teach.eq(targets).sum().item(), criterion(outputs, targets).item()))\n",
        "        print('Student correct out of {}: {}, loss at {}'.format(bs, predicted_std.eq(targets).sum().item(), criterion(outputs_student, targets).item()))\n",
        "\n",
        "    name_use = \"best_images\"\n",
        "    if prefix is not None:\n",
        "        name_use = prefix + name_use\n",
        "    next_batch = len(glob.glob(\"./%s/*.png\" % name_use)) // 1\n",
        "\n",
        "    #vutils.save_image(best_inputs[:20].clone(),\n",
        "     #                 './{}/output_{}.png'.format(name_use, next_batch),\n",
        "      #                normalize=True, scale_each = True, nrow=10)\n",
        "\n",
        "    #if train_writer is not None:\n",
        "     #   train_writer.add_scalar('gener_teacher_criteria', criterion(outputs, targets), global_iteration)\n",
        "      #  train_writer.add_scalar('gener_student_criteria', criterion(outputs_student, targets), global_iteration)\n",
        "\n",
        "       # train_writer.add_scalar('gener_teacher_acc', predicted_teach.eq(targets).sum().item() / bs, global_iteration)\n",
        "       # train_writer.add_scalar('gener_student_acc', predicted_std.eq(targets).sum().item() / bs, global_iteration)\n",
        "\n",
        "        #train_writer.add_scalar('gener_loss_total', loss.item(), global_iteration)\n",
        "        #train_writer.add_scalar('gener_loss_var', (var_scale*loss_var).item(), global_iteration)\n",
        "\n",
        "    net_student.train()\n",
        "\n",
        "    return best_inputs"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please install apex from https://www.github.com/nvidia/apex to run this example.\n",
            "will attempt to run without it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "6231bc717dd5453785f99a33b5dda114",
            "5b78f85b903a4e5f8b4fc19632a633e1",
            "d56cb565f93e4280a06cd0af9cd3af9b",
            "b9fc6f5a2b0941c8a251c623ddc65ec7",
            "7091c37439f34b18b295c75c0f91c4bf",
            "a271acccf6674cc892a9c1b6da6b0c5e",
            "5f710ed00c0e4f969f1e8c582e5dab68",
            "11e465749718416c88f608299920f38b"
          ]
        },
        "id": "Ap6fvXN9cB98",
        "outputId": "ee78d32c-0f17-4e56-9590-a68e6dafbb7d"
      },
      "source": [
        "method = mnemonics(randomseed=203)\n",
        "model, batch = method.trainer()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "current batches [11, 5, 62, 76, 27, 3, 96, 33, 78, 30]\n",
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6231bc717dd5453785f99a33b5dda114",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "8144ac86d5c44d8fa4eaac462a9dfa42",
            "6528317c3c474513b49c0c8936c564d5",
            "fb1cfb103f5d4ddea923785d2df5c976",
            "79f71bc6fc6346eb8333ab4acbefac00",
            "00e4d39278244ef79422af219a367494",
            "c58899c0a1f9482382eeac46b4456603",
            "bb1701dedc8c452b889028491781f4e3",
            "d12aa3338fc743a58cf6771ad89ba144"
          ]
        },
        "id": "vdgNQmOTcD_V",
        "outputId": "5dd20f14-fb8a-4e1c-d15a-22489b361c65"
      },
      "source": [
        "# Train only FC layers -> Freeze convolutional Layers\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "model.fc = nn.Linear(64, 10).to('cuda')\n",
        "\n",
        "train_dataset = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "test_dataset = Subset(ilCIFAR100(10, 203, train = 'test'), ilCIFAR100(10, 203, train = 'test').get_batch_indexes()[0])\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True, num_workers=4, drop_last=True)\n",
        "diz = ilCIFAR100(10, 203, train = 'train').get_dict()\n",
        "\n",
        "\n",
        "# Prepare Training\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9, weight_decay= 1e-5)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[49,63], gamma=0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in tqdm(range(70)):\n",
        "    \n",
        "  for _, inputs, labels in train_loader:\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    labels = torch.tensor([diz[c.item()] for c in labels])\n",
        "\n",
        "    labels=labels.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    outputs=model(inputs)\n",
        "\n",
        "    labels_encoded = F.one_hot(labels,10).float().to('cuda') #CAMBIARE ONE_HOT\n",
        "    loss = criterion(outputs, labels_encoded) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  scheduler.step()\n",
        "\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy', acc)\n",
        "\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8144ac86d5c44d8fa4eaac462a9dfa42",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test accuracy 0.83203125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFFN1KCJaQ2o",
        "outputId": "fad0a35c-61d6-4818-8f9f-49ddbbc44d23"
      },
      "source": [
        "!git clone https://github.com/huyvnphan/PyTorch_CIFAR10.git\n",
        "\n",
        "!pip install pytorch_lightning\n",
        "!python /content/PyTorch_CIFAR10/train.py --download_weights 1\n",
        "\n",
        "! cp -r /content/cifar10_models/state_dicts /content"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PyTorch_CIFAR10'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 637 (delta 39), reused 50 (delta 22), pack-reused 552\u001b[K\n",
            "Receiving objects: 100% (637/637), 6.59 MiB | 20.13 MiB/s, done.\n",
            "Resolving deltas: 100% (222/222), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHiRaF-FbGDD",
        "outputId": "599dd9d8-3b6d-4723-c184-a7056dd92f8e"
      },
      "source": [
        "from resnet import resnet50, resnet18\n",
        "\n",
        "trials = resnet18(pretrained = True).to('cuda')\n",
        "trials.eval()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIsGltQmiHm4"
      },
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = True"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819,
          "referenced_widgets": [
            "5bc7f6ccb80a43eabaf1bdf02e24ece2",
            "d89e4a4b204047d1926bf269fdee14f0",
            "959b6c00d6094263827c1776d7d8da5e",
            "8ef6b65348484c4c8da084f2557a4f4e",
            "5fd36a917203484f8ed6f297ca86d726",
            "779960904ed446c7b06a430f38b2f5f5",
            "92848c6c36844570b6657ffd6a305a8f",
            "2e897941ec694ee1a1d8b87fa7b5c77f"
          ]
        },
        "id": "GgHHC5lGcOwH",
        "outputId": "380caeed-b554-4bcb-91f5-ed6d8e80b523"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/NVlabs/DeepInversion.git\n",
        "\n",
        "!pip install tensorboardX\n",
        "! cp -r /content/DeepInversion/cifar10/deepinversion_cifar10.py /content\n",
        "! cp -r /content/DeepInversion/cifar10/resnet_cifar.py /content\n",
        "'''\n",
        "#from resnet_cifar import ResNet18 # HO IMPORTATO LA CLASSE DEL PAPER E PROVATO CON LA LORO RESNET 18 (non cambia nulla)\n",
        "labels_of_modified = torch.zeros(0, dtype = int).to('cuda')\n",
        "for label in batch:\n",
        "  labels = torch.LongTensor([diz[label]]*20).to('cuda')\n",
        "  labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\n",
        "print('len to be created', len(labels_of_modified))\n",
        "number_of_images_created = 200\n",
        "\n",
        "teacher = copy.deepcopy(model)\n",
        "net_teacher = resnet32(num_classes=10).to('cuda')\n",
        "net_teacher.load_state_dict(teacher.state_dict())\n",
        "#net_student = resnet32(num_classes=10).to('cuda')\n",
        "#net_student = ResNet18().to('cuda')\n",
        "net_student = resnet18().to('cuda')\n",
        "net_teacher.eval()\n",
        "\n",
        "inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device='cuda', dtype=torch.float)\n",
        "train_writer = None  # tensorboard writter\n",
        "global_iteration = 0\n",
        "di_lr = 0.05\n",
        "optimizer_di = optim.Adam([inputs], lr=di_lr)\n",
        "plt.imshow(tensor2im(inputs[0]))\n",
        "plt.show()\n",
        "print(\"Starting model inversion\")\n",
        "batch_idx = 0\n",
        "inputs = get_images(net=trials, bs=len(labels_of_modified), epochs=1500, idx=batch_idx, \n",
        "                  net_student=net_student, competitive_scale=10.0, l2_coeff = 3e-8, bn_reg_scale = 5.0,\n",
        "                  train_writer=train_writer, use_amp=False,\n",
        "                  optimizer=optimizer_di, inputs=inputs, \n",
        "                  var_scale=0.001, labels=labels_of_modified) #2.5e-5\n",
        "\n",
        "plt.imshow(tensor2im(inputs[0]))\n",
        "plt.show()\n",
        "print('deepinversion finshed')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len to be created 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Starting model inversion\n",
            "Teacher correct out of 200: 22, loss at 4.327481269836426\n",
            "Student correct out of 200: 20, loss at 2.5511488914489746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bc7f6ccb80a43eabaf1bdf02e24ece2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "It 0\t Losses: total: 30.352,\ttarget: 4.387 \tR_feature_loss unscaled:\t 2.434\n",
            "It 200\t Losses: total: 11.201,\ttarget: 0.004 \tR_feature_loss unscaled:\t 0.358\n",
            "It 400\t Losses: total: 10.687,\ttarget: 0.003 \tR_feature_loss unscaled:\t 0.240\n",
            "It 600\t Losses: total: 10.187,\ttarget: 0.003 \tR_feature_loss unscaled:\t 0.196\n",
            "It 800\t Losses: total: 10.218,\ttarget: 0.003 \tR_feature_loss unscaled:\t 0.177\n",
            "It 1000\t Losses: total: 10.155,\ttarget: 0.003 \tR_feature_loss unscaled:\t 0.178\n",
            "It 1200\t Losses: total: 9.915,\ttarget: 0.002 \tR_feature_loss unscaled:\t 0.184\n",
            "It 1400\t Losses: total: 10.038,\ttarget: 0.003 \tR_feature_loss unscaled:\t 0.196\n",
            "\n",
            "Teacher correct out of 200: 200, loss at 0.0023662541061639786\n",
            "Student correct out of 200: 0, loss at 3.093940019607544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "deepinversion finshed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "oPaHI4DVczH9",
        "outputId": "871f317a-d3a7-47f0-b3b9-20fdd4e23980"
      },
      "source": [
        "plt.imshow(tensor2im(inputs[98]))\n",
        "plt.show()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "lL_1rQuLlppK",
        "outputId": "9cba50ec-bd77-496e-f71a-29c9fa0dd1bb"
      },
      "source": [
        "plt.imshow(tensor2im(inputs[98]))\n",
        "plt.show()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EucHkMdcqJkH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}