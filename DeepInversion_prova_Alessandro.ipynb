{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepInversion_prova_Alessandro.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNoMc80n3q88sRMGd+jqsJv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cc66af5dc6354f27a94e0cf10ee4b939": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c25e865988494b32b57c559e0f3176c4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_adfc6e61cb8b4f45afeff11e6670ba20",
              "IPY_MODEL_d962e73d2159493c840df67b6232dc18"
            ]
          }
        },
        "c25e865988494b32b57c559e0f3176c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "adfc6e61cb8b4f45afeff11e6670ba20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3dcb4216c04e469f851881831d6e9ab3",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 70,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 70,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aae5570542824ef09f0e3ce25dad4cdd"
          }
        },
        "d962e73d2159493c840df67b6232dc18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1ed95664efcf4ea3ba069ccd54c09440",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 70/70 [05:29&lt;00:00,  4.70s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_797674f3e28d4574af33a380305ae417"
          }
        },
        "3dcb4216c04e469f851881831d6e9ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aae5570542824ef09f0e3ce25dad4cdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1ed95664efcf4ea3ba069ccd54c09440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "797674f3e28d4574af33a380305ae417": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ea7c673275c4e0e814669d8171241a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f4667a3af1dc4812a6120806f8c21b35",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a628dfd4459449219e9bec1cd809abd3",
              "IPY_MODEL_3df610f48fde44c5bcf48b14a22e1ac4"
            ]
          }
        },
        "f4667a3af1dc4812a6120806f8c21b35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a628dfd4459449219e9bec1cd809abd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fe644c3778444c55b317f56938747695",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1500,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1500,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4e210f0ebc4d4023816c57912ca93cf9"
          }
        },
        "3df610f48fde44c5bcf48b14a22e1ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ca0bba673b2b40b79545d002368148d9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1500/1500 [42:55&lt;00:00,  1.72s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_02a737261a8646fda2536c466f9a8fa3"
          }
        },
        "fe644c3778444c55b317f56938747695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4e210f0ebc4d4023816c57912ca93cf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ca0bba673b2b40b79545d002368148d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "02a737261a8646fda2536c466f9a8fa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlePa98/DeepInversion/blob/master/DeepInversion_prova_Alessandro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfqenFowb1xL",
        "outputId": "f3e0e3d0-817f-4bc3-c1f2-da613fbcc8d8"
      },
      "source": [
        "!pip3 install 'import_ipynb'\n",
        "!pip3 install 'tqdm'\n",
        " \n",
        "!rm -r IncrementalLearning\n",
        "# upload work files from your git hub repository\n",
        "import sys\n",
        " \n",
        "!git clone https://github.com/alessandronicolini/IncrementalLearning.git # clone proj repository\n",
        "!rm -rf IncrementalLearning/README.md \n",
        "!rm -rf IncrementalLearning/baselines.ipynb\n",
        " \n",
        "path = 'IncrementalLearning/'\n",
        "if path not in sys.path:\n",
        "    sys.path.append('IncrementalLearning/')\n",
        " \n",
        "!pip3 install import_ipynb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import_ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp36-none-any.whl size=2976 sha256=3f957495c759d07f9d5d8a8ca8c17f4f5408aa655744dee3c62f9bca2f72665d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "rm: cannot remove 'IncrementalLearning': No such file or directory\n",
            "Cloning into 'IncrementalLearning'...\n",
            "remote: Enumerating objects: 166, done.\u001b[K\n",
            "remote: Counting objects: 100% (166/166), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 640 (delta 102), reused 0 (delta 0), pack-reused 474\u001b[K\n",
            "Receiving objects: 100% (640/640), 860.31 KiB | 8.35 MiB/s, done.\n",
            "Resolving deltas: 100% (377/377), done.\n",
            "Requirement already satisfied: import_ipynb in /usr/local/lib/python3.6/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us1alRIbb5pW"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VisionDataset\n",
        "from PIL import Image\n",
        "import random\n",
        "import torchvision\n",
        "ROOT = './data'\n",
        "class ilCIFAR100(VisionDataset):\n",
        "    \"\"\"\n",
        "    Extends CIFAR100 class. Split the dataset into 10 batches, each one containing 10 classes.\n",
        "    You can retrieve the batches from the attribute \"batches\", it has different structure according to\n",
        "    test and train CIFAR100 splits:\n",
        "        - train -> batches is a dictionary {0:{'train':indexes, 'val':indexes}...} \n",
        "        - test -> batches is a dictionary {0:indexes...}\n",
        "    where the keys are the batch number.\n",
        "\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory\n",
        "            `cifar-10-batches-py` exists or will be saved to if download is set to True.\n",
        "        seed(int): used to ensure reproducibility in shuffling operations.\n",
        "        val_size(float, optional): between 0 and 1, fraction of data used for validation.\n",
        "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
        "            creates from test set.\n",
        "        transform (callable, optional): A function/transform that takes in an PIL image\n",
        "            and returns a transformed version. E.g, `transforms.RandomCrop`\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "    \"\"\"\n",
        "    def __init__(self,classes_per_batch, seed, val_size=0.1, train=True, transform=None, target_transform=False, \n",
        "    download=True):\n",
        "        \n",
        "        super(ilCIFAR100, self).__init__(root=0)\n",
        "        self.classes_per_batch=classes_per_batch\n",
        "\n",
        "        \n",
        "        self.__rs = seed # set random seed \n",
        "        self.train=train\n",
        "        self.__transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "\n",
        "        self.__transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "        ])\n",
        "        # if train:\n",
        "        #     self.batches = self.__make_train_batches(val_size)\n",
        "        # else:\n",
        "        #     self.batches = self.__make_test_batches()\n",
        "        if self.train == 'train':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self.__transform_train)\n",
        "        elif self.train == 'exemplars':\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=True,\n",
        "                                            download=True, transform=self.__transform_test)\n",
        "        else:\n",
        "          self.dataset = torchvision.datasets.CIFAR100(root=ROOT, train=False,\n",
        "                                        download=True, transform=self.__transform_test)\n",
        "          \n",
        "        self.targets = np.array(self.dataset.targets) # make targets an array to exploit masking\n",
        "        random.seed(seed)\n",
        "        self.classes = random.sample(range(0, 100), 100)\n",
        "        #self.classes = self.classes.reshape((10, -1)) # each row contains the classes for the corrisponding batch\n",
        "        #print(self.classes)\n",
        "        self.__dictionary = {}\n",
        "        for i, c in enumerate(self.classes):\n",
        "          self.__dictionary[c] = i\n",
        "\n",
        "\n",
        "    def get_dict(self):\n",
        "      return self.__dictionary\n",
        "    def __getitem__(self, index):\n",
        "        return index,self.dataset.__getitem__(index)[0],self.dataset.__getitem__(index)[1]\n",
        "    def __len__(self):\n",
        "        return self.dataset.__len__()\n",
        "    def getbatches(self):\n",
        "      classlist=self.classes\n",
        "      batches=[]\n",
        "      for i in range(0,int(100/self.classes_per_batch)):\n",
        "        #print(i)\n",
        "        batch=classlist[int(i*self.classes_per_batch):int(i*10+self.classes_per_batch)]\n",
        "        batches.append(batch)\n",
        "      return batches\n",
        "    def get_batch_indexes(self):\n",
        "      classlist=self.classes\n",
        "      numclass=self.classes_per_batch\n",
        "      batch_indexes=[]\n",
        "      for i in range(0,int(100/self.classes_per_batch)):\n",
        "        batch=classlist[int(i*numclass):int(i*numclass+numclass)]\n",
        "        mask=np.isin(self.targets,batch)\n",
        "        indexes=np.array(np.arange(len(self.dataset.targets)))\n",
        "        indexes=indexes[mask]\n",
        "        batch_indexes.append(indexes)\n",
        "      return batch_indexes\n",
        "    def get_class_indexes(self,label):\n",
        "      indexes = np.array(np.arange(len(self.dataset.targets)))\n",
        "      labels = self.dataset.targets\n",
        "      mask = np.isin(labels, label)\n",
        "      indexes = indexes[mask]\n",
        "\n",
        "      return indexes\n",
        "    def get_train_val(self,valid):\n",
        "      batches=self.get_batch_indexes()\n",
        "      train=[]\n",
        "      val=[]\n",
        "      for batch in batches:\n",
        "        #print(type(batch))\n",
        "        random.shuffle(batch)\n",
        "        valbatch=batch[0:int(valid*len(batch))]\n",
        "        trainbatch=batch[int(valid*len(batch)):]\n",
        "        train.append(trainbatch)\n",
        "        val.append(valbatch)\n",
        "      return train,val\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import time\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    #CIFAR100\n",
        "    mean = [0.5071, 0.4867, 0.4408] \n",
        "    std = [0.2675, 0.2565, 0.2761]\n",
        "    #CIFAR10\n",
        "    #mean = [0.4914, 0.4822, 0.4465]\n",
        "    #std = [0.2023, 0.1994, 0.2010]\n",
        "    if not isinstance(input_image, np.ndarray):\n",
        "        if isinstance(input_image, torch.Tensor):\n",
        "            image_tensor = input_image.data\n",
        "        else:\n",
        "            return input_image\n",
        "        image_numpy = image_tensor.cpu().detach().float().numpy()\n",
        "        if image_numpy.shape[0] == 1:\n",
        "            image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "        for i in range(len(mean)): \n",
        "            image_numpy[i] = image_numpy[i] * std[i] + mean[i]\n",
        "        image_numpy = image_numpy * 255\n",
        "        image_numpy = np.transpose(image_numpy, (1, 2, 0))\n",
        "    else:\n",
        "        image_numpy = input_image\n",
        "    return image_numpy.astype(imtype)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTxZa5_vb7x-",
        "outputId": "d6ab4a90-3cdf-4c17-8d97-2f6a03d21250"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "from sklearn.preprocessing import normalize\n",
        "import copy\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.models as models\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from torch.utils.data import Subset, DataLoader, Dataset\n",
        "import random\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "import import_ipynb\n",
        "#from IncrementalLearning.cifar100 import ilCIFAR100\n",
        "\n",
        "from IncrementalLearning.resnet_cifar import resnet32\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "importing Jupyter notebook from /content/IncrementalLearning/resnet_cifar.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "E_tW2Uqnb_wT",
        "outputId": "2f5d0142-66fa-42e4-c35d-8139f60dad2a"
      },
      "source": [
        "class mnemonics():\n",
        "  def __init__(self, randomseed):\n",
        "    self.device = 'cuda'\n",
        "    self.model = resnet32(num_classes=100).to(self.device)\n",
        "    self.feature_extractor = self.model.features\n",
        "    self.temp_model = None\n",
        "    self.lr = 2\n",
        "    self.gamma = 0.2\n",
        "    self.weight_decay = 1e-5 \n",
        "    self.milestones = [49,63]\n",
        "    self.batch_size = 128\n",
        "    self.numepochs = 70\n",
        "    self.n_classes = 0\n",
        "    self.n_known = 0\n",
        "    self.feature_size=64\n",
        "    self.momentum=0.9\n",
        "    self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    self.NUM_BATCHES=10\n",
        "    self.randomseed=randomseed\n",
        "    self.trainloader=None\n",
        "    self.testloader=None\n",
        "    self.CLASSES_PER_BATCH=10\n",
        "\n",
        "    self.original_training_set = ilCIFAR100(self.CLASSES_PER_BATCH, self.randomseed, train = 'train')\n",
        "    self.original_exemplar_set = ilCIFAR100(self.CLASSES_PER_BATCH, self.randomseed, train = 'exemplars')\n",
        "    self.original_test_set = ilCIFAR100(self.CLASSES_PER_BATCH,self.randomseed, train= 'test')\n",
        "\n",
        "    self.last_test = None\n",
        "    self.y_pred = []\n",
        "    self.y_test = []\n",
        "\n",
        "    self.cumulative_class_mean = []\n",
        "\n",
        "    self.classes_seen=0\n",
        "    self.diz = self.original_training_set.get_dict()\n",
        "\n",
        "    self.exemplar_features_mean = None\n",
        "    # lista di liste, ogni lista contiene gli exemplars di una classe\n",
        "    self.exemplar_sets_idxs = [] # mn_exemplat_sets\n",
        "    # lista unica, tutti gli indici degli exemplar\n",
        "    self.exemplar_idxs = []\n",
        "\n",
        "  '''\n",
        "  def update_params(self, \n",
        "                    m,\n",
        "                    finetuning_idxs, \n",
        "                    training_idxs, \n",
        "                    mnemonics_to_optimize, \n",
        "                    batch_size,\n",
        "                    new=True,\n",
        "                    lr=10, \n",
        "                    momentum=0.9, \n",
        "                    weight_decay=1e-5, \n",
        "                    milestones=[10, 20, 30, 40],\n",
        "                    gamma=0.5, \n",
        "                    tuning_epochs=4,\n",
        "                    updating_epochs=50):\n",
        "    \n",
        "    \"\"\"\n",
        "    finetuning_idxs = indexes of current task elements\n",
        "    mnemonics_idxs = indexes of exemplar elements\n",
        "    mnemonics_to_optimize = the optimized parameters in the update phase\n",
        "    \"\"\"\n",
        "\n",
        "    # make a copy of the model\n",
        "    model_copy = copy.deepcopy(self.model)\n",
        "    model_copy.train()\n",
        "    model_copy.to(self.device)\n",
        "\n",
        "    # define the loss\n",
        "    # criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # FINE TUNING FOR 1 EPOCH eq. 8 --------------------------------------------\n",
        "    \n",
        "    # define optimizer and scheduler for fine tuning phase\n",
        "    optimizer = optim.SGD(model_copy.parameters(), lr=2, momentum=momentum, weight_decay=weight_decay)\n",
        "    \n",
        "    # create the subset dataset to load the data you want, and the loader\n",
        "    finetuning_labels = np.array([self.original_training_set.__getitem__(idx)[2] for idx in finetuning_idxs], dtype=int)\n",
        "    meta_idxs = [i for i in range(len(finetuning_idxs))]\n",
        "    random.shuffle(meta_idxs)\n",
        "\n",
        "    # split the meta idxs in batches\n",
        "    n_batches = int(np.floor(len(finetuning_idxs)/batch_size))\n",
        "    meta_idxs_batches = []\n",
        "    for i in range(n_batches):\n",
        "      meta_idxs_batches.append(np.array(meta_idxs[batch_size*i:batch_size*(i+1)]))\n",
        "    meta_idxs_batches.append(np.array(meta_idxs[batch_size*n_batches:]))\n",
        "\n",
        "    # now fine tune the copied model\n",
        "    for epoch in range(tuning_epochs):\n",
        "      for meta_idxs_batch in meta_idxs_batches:\n",
        "        inputs = mnemonics_to_optimize[0][meta_idxs_batch] # are already in cuda\n",
        "        labels = finetuning_labels[meta_idxs_batch]\n",
        "        labels = torch.tensor([self.diz[c] for c in labels])\n",
        "        labels = labels.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_copy(inputs)\n",
        "        labels_encoded = F.one_hot(labels,100).float().to(self.device)\n",
        "        loss = self.criterion(outputs, labels_encoded)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "\n",
        "    # UPDATE THE MNEMONICS eq.9/10 ---------------------------------------------\n",
        "    \n",
        "    model_copy.eval()\n",
        "    \n",
        "    optimizer = optim.SGD(mnemonics_to_optimize, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
        "    \n",
        "\n",
        "    if new:\n",
        "      exlvl_training = Subset(self.original_training_set, training_idxs)\n",
        "      exlvl_loader = DataLoader(exlvl_training, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "      current_task_labels = set([self.original_training_set.__getitem__(idx)[2] for idx in training_idxs])\n",
        "      new_dict = {label:new_label for label, new_label in zip(current_task_labels, range(10))}\n",
        "\n",
        "      new_class_mean = {new_dict[key] : value for key, value in self.cumulative_class_mean.items()}\n",
        "      means_ready = torch.Tensor(list(new_class_mean.values())).to(self.device)\n",
        "\n",
        "    \n",
        "    else:\n",
        "      exlvl_training = Subset(self.original_exemplar_set, training_idxs)\n",
        "      exlvl_loader = DataLoader(exlvl_training, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    print('lunghezza del exemplar update:', len(training_idxs))\n",
        "    for epoch in tqdm(range(updating_epochs)):\n",
        "\n",
        "      for _, inputs, labels in exlvl_loader:\n",
        "\n",
        "        if new:\n",
        "          labels = torch.tensor([new_dict[c.item()] for c in labels])\n",
        "        else:\n",
        "          labels = torch.tensor([self.diz[c.item()] for c in labels])\n",
        "\n",
        "        labels = labels.to(self.device)\n",
        "        inputs = inputs.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        out_features = model_copy.features(inputs)\n",
        "        # compute features mean of mnemonics for each class\n",
        "        if new:\n",
        "          the_logits = F.linear(F.normalize(out_features, p=2, dim=1), F.normalize(means_ready, p=2, dim=1))\n",
        "        else:\n",
        "          ##Da capire questa cosa!!!!!!! se provassi a mettere self?\n",
        "          #OPPURE uso un altro tensore copiando all_class_means_calcolato_fuori?  \n",
        "          #dov'è il gradiente?\n",
        "          n_classes = int(len(finetuning_labels)/m)\n",
        "          all_class_means = torch.zeros((0, 64))\n",
        "          all_class_means = all_class_means.to(self.device)\n",
        "          for i in range(n_classes): # how many classes\n",
        "            mnemonics_features = model_copy.features(mnemonics_to_optimize[0][i*m:(i+1)*m])\n",
        "            this_class_means = torch.mean(mnemonics_features, dim=0) # size 64\n",
        "            this_class_means = torch.unsqueeze(this_class_means, dim=0) # add the second dimension\n",
        "            all_class_means = torch.cat((all_class_means, this_class_means), dim=0)\n",
        "          the_logits = F.linear(F.normalize(out_features, p=2, dim=1), F.normalize(all_class_means, p=2, dim=1))\n",
        "\n",
        "        #labels_encoded = F.one_hot(labels,100).float().cuda()\n",
        "\n",
        "        loss = F.cross_entropy(the_logits, labels) # al secondo batch di classi per i new mnemonics le uscite sono sempre 10 ma le label vanno da 10 a 19\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "  def exemplar_level_optimization(self, m, task_num, current_task_indices):  \n",
        "    \n",
        "    # UPDATING NEW EXEMPLAR-----------------------------------------------------\n",
        "\n",
        "    # isola gli indici dei nuovi exemplars\n",
        "    new_exemplar_idxs = []\n",
        "    for idxs in self.exemplar_sets_idxs[-10:]:\n",
        "      new_exemplar_idxs += idxs\n",
        "\n",
        "    # ora ottieni gli mnemonics che poi sono da ottimizzare\n",
        "    new_mnemonics_data = torch.zeros((10*m, 3, 32, 32))\n",
        "\n",
        "    for i, idx in enumerate(new_exemplar_idxs):\n",
        "      new_mnemonics_data[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "\n",
        "    new_mnemonics = nn.ParameterList()\n",
        "    new_mnemonics.append(nn.Parameter(new_mnemonics_data))\n",
        "    new_mnemonics.to(self.device)\n",
        "    \n",
        "    #print(new_mnemonics[0][0])\n",
        "\n",
        "    options_new ={'finetuning_idxs': new_exemplar_idxs, \n",
        "                  'training_idxs': current_task_indices, \n",
        "                  'mnemonics_to_optimize':  new_mnemonics,  \n",
        "                  'batch_size':128,\n",
        "                  'm':m}\n",
        "\n",
        "    print('---start mnemonics updating---')\n",
        "\n",
        "    self.update_params(**options_new)    \n",
        "\n",
        "    for i, idx in enumerate(new_exemplar_idxs):\n",
        "      self.original_exemplar_set.dataset.data[idx] = tensor2im(new_mnemonics[0][i])\n",
        "\n",
        "    \n",
        "    # UPDATING OLD EXEMPLARS ---------------------------------------------------\n",
        "\n",
        "    if task_num:\n",
        "      # decidi quanti elementi ha ogni exemlar set in a e in b a seconda se m è \n",
        "      # pari o dispari\n",
        "      if m%2:\n",
        "        l_a = int((m+1)/2)\n",
        "      else:\n",
        "        l_a = int(m/2)\n",
        "      l_b = int(m-l_a)\n",
        "\n",
        "      # isola gli indici dei vecchi exemplars, dividendoli in due parti\n",
        "      # ogni classe deve avere circa la metà degli exemplar originali\n",
        "      old_exemplar_idxs_a = []\n",
        "      old_exemplar_idxs_b = []\n",
        "      \n",
        "      for idxs in self.exemplar_sets_idxs[:-10]:\n",
        "        old_exemplar_idxs_a += idxs[:l_a]\n",
        "        old_exemplar_idxs_b += idxs[l_a:]\n",
        "\n",
        "      old_mnemonics_data_a = torch.zeros((task_num*10*l_a, 3, 32, 32))\n",
        "      old_mnemonics_data_b = torch.zeros((task_num*10*l_b, 3, 32, 32))\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_a):\n",
        "        old_mnemonics_data_a[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "      \n",
        "      for i, idx in enumerate(old_exemplar_idxs_b):\n",
        "          old_mnemonics_data_b[i, :, :, :] = self.original_training_set.__getitem__(int(idx))[1]\n",
        "      \n",
        "      old_mnemonics_a = nn.ParameterList()\n",
        "      old_mnemonics_a.append(nn.Parameter(old_mnemonics_data_a))\n",
        "      old_mnemonics_a.to(self.device)\n",
        "      old_mnemonics_b = nn.ParameterList()\n",
        "      old_mnemonics_b.append(nn.Parameter(old_mnemonics_data_b))\n",
        "      old_mnemonics_b.to(self.device)\n",
        "\n",
        "      options_old_a = {'finetuning_idxs':old_exemplar_idxs_a, \n",
        "                       'training_idxs':old_exemplar_idxs_b, \n",
        "                       'mnemonics_to_optimize':old_mnemonics_a, \n",
        "                       'batch_size':128,\n",
        "                       'm': l_a,\n",
        "                       'new':False}\n",
        "\n",
        "      options_old_b = {'finetuning_idxs':old_exemplar_idxs_b, \n",
        "                       'training_idxs':old_exemplar_idxs_a, \n",
        "                       'mnemonics_to_optimize':old_mnemonics_b, \n",
        "                       'batch_size':128,\n",
        "                       'm':l_b,\n",
        "                       'new':False}\n",
        "\n",
        "      self.update_params(**options_old_a) \n",
        "      self.update_params(**options_old_b)\n",
        "\n",
        "      # CONVERT AND STORE UPDATED EXEMPLAR as numpy array\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_a):\n",
        "        self.original_exemplar_set.dataset.data[idx] = tensor2im(old_mnemonics_a[0][i])\n",
        "\n",
        "      for i, idx in enumerate(old_exemplar_idxs_b):\n",
        "        self.original_exemplar_set.dataset.data[idx] = tensor2im(old_mnemonics_b[0][i])\n",
        "    \n",
        "\n",
        "\n",
        "    # FINE TUNE THE CURRENT NET ON ALL THE EXEMPLARS COLLECTED 'TILL NOW\n",
        "   \n",
        "  '''\n",
        "\n",
        "  def model_level_optimization(self):\n",
        "    \n",
        "    old_model = copy.deepcopy(self.model)\n",
        "    old_model.eval()\n",
        "    old_model.to(self.device)\n",
        "    n_classes = self.classes_seen+self.CLASSES_PER_BATCH\n",
        "    print(n_classes)\n",
        "    optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=self.momentum, weight_decay=self.weight_decay)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.milestones, gamma=self.gamma)\n",
        "    for epoch in tqdm(range(self.numepochs)):\n",
        "        \n",
        "      for _, inputs, labels in self.trainloader:\n",
        "        inputs = inputs.float().to(self.device)\n",
        "        labels = torch.tensor([self.diz[c.item()] for c in labels])\n",
        "\n",
        "        labels=labels.to(self.device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs=self.model(inputs)\n",
        "\n",
        "        labels_encoded = F.one_hot(labels,100).float().to(self.device) #CAMBIARE ONE_HOT\n",
        "        \n",
        "        if self.classes_seen:\n",
        "          old_target = old_model(inputs).to(self.device)\n",
        "          old_target = torch.sigmoid(old_target).to(self.device)\n",
        "          \n",
        "          target = torch.cat((old_target[:,:self.classes_seen], labels_encoded[:, self.classes_seen:]), dim=1)\n",
        "          loss = self.criterion(outputs, target)\n",
        "        else:\n",
        "          loss = self.criterion(outputs, labels_encoded) \n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "      \n",
        "      scheduler.step()\n",
        "\n",
        "\n",
        "  def classify_nme(self, input_batch):\n",
        "    min_distances = float('inf')*torch.ones(len(input_batch)).to(self.device) # shape: batch_size --> 128\n",
        "    y_pred = torch.zeros(len(input_batch), dtype=torch.int8).to(self.device) # shape: batch_size --> 128\n",
        "    input_features = self.model.features(input_batch) # shape: (batch_size, feature_size) --> (128, 64)\n",
        "\n",
        "    for i in range(len(self.exemplar_sets_idxs)):\n",
        "      ex_mean = self.exemplar_means[i,:]\n",
        "\n",
        "      # compute distances between inputs features and exemplar set means\n",
        "      pdist = nn.PairwiseDistance(p=2)\n",
        "      distances = pdist(input_features, ex_mean) # shape: batch_size --> 128\n",
        "\n",
        "      # update min distancies and predicted labels\n",
        "      mask = distances < min_distances\n",
        "      min_distances[mask] = distances[mask]\n",
        "      y_pred[mask] = self.exemplar_labels[i]\n",
        "\n",
        "    return y_pred\n",
        "    \n",
        "\n",
        "\n",
        "  def get_new_exemplars(self, batch, m):\n",
        "    loader = torch.utils.data.DataLoader(batch, batch_size=self.batch_size,shuffle=False, num_workers=4)\n",
        "    features = np.zeros((0,self.feature_size))\n",
        "    indices = np.zeros((0), dtype=int)\n",
        "    with torch.no_grad():\n",
        "      for indexes, images, labels in loader:\n",
        "        images = images.cuda()\n",
        "        feature = self.feature_extractor(images).data.cpu().numpy()\n",
        "        feature = normalize(feature, axis=1, norm='l2')\n",
        "        features = np.concatenate((features,feature), axis=0)\n",
        "        indices = np.concatenate((indices,indexes), axis=0)\n",
        "\n",
        "    class_mean = np.mean(features, axis=0)\n",
        "    class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n",
        "\n",
        "    #self.cumulative_class_mean.append(class_mean)\n",
        "\n",
        "    exemplar_set = []\n",
        "    exemplar_features = np.zeros((0,self.feature_size))\n",
        "\n",
        "    for k in range(1, int(m)+1):\n",
        "        S = np.sum(exemplar_features, axis=0)\n",
        "        phi = features\n",
        "        mu = class_mean\n",
        "        mu_p = 1.0 / k * (phi + S)\n",
        "        mu_p = normalize(mu_p, axis=1, norm='l2')\n",
        "        i = np.argmin(np.sqrt(np.sum((mu - mu_p) ** 2, axis=1)))\n",
        "        exemplar_set.append(int(indices[i]))\n",
        "        addfeature =  np.expand_dims(features[i], axis=0)\n",
        "        exemplar_features = np.concatenate((exemplar_features,addfeature), axis=0)\n",
        "\n",
        "        #remove duplicates\n",
        "        features = np.delete(features, i, 0)\n",
        "        indices = np.delete(indices, i, 0)\n",
        "        \n",
        "    self.exemplar_sets_idxs.append(exemplar_set)\n",
        "    #self.exemplar_sets_idxs.append(random.sample(list(batch), m))\n",
        "\n",
        "\n",
        "  def reduce_old_exemplars(self, m):\n",
        "    for i, set_i in enumerate(self.exemplar_sets_idxs):\n",
        "      self.exemplar_sets_idxs[i] = random.sample(set_i, m)\n",
        "\n",
        "  def __accuracy_fc(self, dl, mapper):\n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    for  _, images, labels in dl:\n",
        "      labels = torch.tensor([torch.tensor(mapper[c.item()]) for c in labels])\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      outputs = self.model(images)\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def __accuracy_nme(self, dl):\n",
        "    \n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    \n",
        "    for  _, images, labels in dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      preds = self.classify_nme(images)\n",
        "      total += len(labels)\n",
        "      correct += torch.sum(preds == labels).item()\n",
        "\n",
        "      if self.last_test:\n",
        "        self.y_pred += preds.tolist()\n",
        "        self.y_test += labels.tolist()\n",
        "\n",
        "    acc = correct / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "  def plot_confusion_matrix(self):\n",
        " \n",
        "    cm = confusion_matrix(self.y_test, self.y_pred)\n",
        "    cm = np.log(cm+1)\n",
        "    fig, ax = plt.subplots(figsize=(7,7))\n",
        "    sns.heatmap(cm, square=True, cbar=False, ax=ax, cmap=plt.get_cmap('seismic'))\n",
        "    ax.set_xticks(np.linspace(19,99,5))\n",
        "    ax.set_yticks(np.linspace(19,99,5))\n",
        "    ax.set_xticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_yticklabels([20,40,60,80,100], rotation=0)\n",
        "    ax.set_title(\"iCaRL\")\n",
        "    ax.set_xlabel(\"Predicted class\")\n",
        "    ax.set_ylabel(\"True class\")\n",
        "    plt.savefig(\"iCaRL_\"+str(self.randomseed)+\"_cm.png\")\n",
        "    plt.show()\n",
        "    return cm\n",
        "\n",
        "  def plot_data(self, train_dl):\n",
        "\n",
        "    from sklearn.manifold import TSNE\n",
        "    print('------plot data------')\n",
        "\n",
        "    #Data points\n",
        "    train_labels_array = torch.zeros(0).to('cuda')\n",
        "    train_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in train_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      train_dataset_to_reduce = np.concatenate((train_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      train_labels_array = torch.cat((train_labels_array, labels))\n",
        "\n",
        "    \n",
        "    #EX e MN loaders \n",
        "    current_exemplar_indices = np.array([], dtype=int)\n",
        "\n",
        "    for exemplar_set in self.exemplar_sets_idxs:\n",
        "      current_exemplar_indices = np.concatenate([current_exemplar_indices, np.array(exemplar_set)])\n",
        "\n",
        "    exemplar_dataset = Subset(self.original_training_set, current_exemplar_indices)\n",
        "    ex_dl = DataLoader(exemplar_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) #usato solo per plottare\n",
        "\n",
        "    mn_dataset = Subset(self.original_exemplar_set, current_exemplar_indices)\n",
        "    mn_dl = DataLoader(mn_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) #usato solo per plottare\n",
        "\n",
        "\n",
        "    #Exemplars\n",
        "\n",
        "    ex_labels_array = torch.zeros(0).to('cuda')\n",
        "    ex_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in ex_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      ex_dataset_to_reduce = np.concatenate((ex_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      ex_labels_array = torch.cat((ex_labels_array, labels), dim = 0)\n",
        "\n",
        "\n",
        "    #Mnemonics\n",
        "\n",
        "\n",
        "    mn_labels_array = torch.zeros(0).to('cuda')\n",
        "    mn_dataset_to_reduce = np.zeros((0,64), dtype = float)\n",
        "\n",
        "    for  _, images, labels in mn_dl:\n",
        "      labels = labels.to(self.device)\n",
        "      images = images.to(self.device)\n",
        "      mn_dataset_to_reduce = np.concatenate((mn_dataset_to_reduce, self.feature_extractor(images).cpu().detach().numpy()))\n",
        "      mn_labels_array = torch.cat((mn_labels_array, labels), dim = 0)\n",
        "\n",
        "    #PLOT'''\n",
        "    total_data_w_exemplars = np.concatenate((train_dataset_to_reduce, ex_dataset_to_reduce))\n",
        "    total_data_w_mn =  np.concatenate((train_dataset_to_reduce, mn_dataset_to_reduce))\n",
        "\n",
        "    total_transformed_ex = TSNE(n_components=2).fit_transform(total_data_w_exemplars)\n",
        "    X_transformed_w_ex = total_transformed_ex[:train_dataset_to_reduce.shape[0]]\n",
        "    ex_transformed = total_transformed_ex[train_dataset_to_reduce.shape[0]:]\n",
        "\n",
        "    total_transformed_mn = TSNE(n_components=2).fit_transform(total_data_w_mn)\n",
        "    X_transformed_w_mn = total_transformed_mn[:train_dataset_to_reduce.shape[0]]\n",
        "    mn_transformed = total_transformed_mn[train_dataset_to_reduce.shape[0]:]\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(24,12))\n",
        "    ax1.scatter(X_transformed_w_ex[:,0], X_transformed_w_ex[:,1], c = train_labels_array.cpu(), alpha = 0.1)\n",
        "    ax1.scatter(ex_transformed[:,0], ex_transformed[:,1], c = ex_labels_array.cpu(), alpha = 1)\n",
        "    #ax1.title('EXEMPLARS')\n",
        "\n",
        "    ax2.scatter(X_transformed_w_mn[:,0], X_transformed_w_mn[:,1], c = train_labels_array.cpu(), alpha = 0.1)\n",
        "    ax2.scatter(mn_transformed[:,0], mn_transformed[:,1], c = mn_labels_array.cpu(), alpha = 1)\n",
        "    #ax2.title('MNEMONICS')\n",
        "    plt.show()\n",
        "\n",
        "  def trainer(self):\n",
        "    \n",
        "    train_indices = self.original_training_set.get_batch_indexes()\n",
        "    test_indices = self.original_test_set.get_batch_indexes()\n",
        "    batches=self.original_training_set.getbatches()\n",
        "    current_test_indexes=[]\n",
        "    test_acc = []\n",
        "    self.last_test = False\n",
        "\n",
        "    for i in range(self.NUM_BATCHES):\n",
        "      print('current batches', batches[i])\n",
        "      if i == self.NUM_BATCHES-1:\n",
        "        self.last_test = True\n",
        "\n",
        "      current_exemplar_indices = np.array([], dtype=int)\n",
        "    \n",
        "      for exemplar_set in self.exemplar_sets_idxs:\n",
        "        current_exemplar_indices = np.concatenate([current_exemplar_indices, np.array(exemplar_set)])\n",
        "\n",
        "      exemplar_dataset = Subset(self.original_training_set, current_exemplar_indices) \n",
        "      #DA CAMBIARE CON SELF.ORIGINAL EXEMPLAR SET\n",
        "      if i > 1: #FINETUNING\n",
        "        print('----inizio finetuning----')\n",
        "        print('numbero of classes in the exemplar sets', len(self.exemplar_sets_idxs))\n",
        "        self.numepochs = 10\n",
        "        self.lr = 0.2\n",
        "        temporary_classes_seen = self.classes_seen\n",
        "        self.classes_seen = 0\n",
        "        self.trainloader = DataLoader(exemplar_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True) \n",
        "        print('accuracy on exemplar set before finetuining:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "        self.model.train()\n",
        "        self.model_level_optimization()\n",
        "        #BACK TO THE NORMAL PARAMETERS\n",
        "        self.model.eval()\n",
        "        self.numepochs = 70\n",
        "        self.lr = 2\n",
        "        self.classes_seen = temporary_classes_seen\n",
        "        print('accuracy on exemplar set after finetuining:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "        current_test_acc = self.__accuracy_nme(self.testloader)\n",
        "        print('accuracy on test set after finetuning:', 100*current_test_acc)\n",
        "        print('-----fine finetuning------')\n",
        "        print('-'*80)\n",
        "\n",
        "      train_dataset = Subset(self.original_training_set, train_indices[i])\n",
        "      current_test_indexes += test_indices[i].tolist()\n",
        "      test_dataset = Subset(self.original_test_set,current_test_indexes)\n",
        "      self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "      self.testloader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4, drop_last=True)        \n",
        "      \n",
        "\n",
        "      if i == 0:\n",
        "        self.trainloader = self.train_loader\n",
        "      else:\n",
        "        self.trainloader = DataLoader(torch.utils.data.ConcatDataset([train_dataset, exemplar_dataset]), batch_size=self.batch_size, shuffle=True,\n",
        "          num_workers=4, pin_memory=True)\n",
        "\n",
        "        \n",
        "      self.model.train()\n",
        "      self.model_level_optimization()    \n",
        "      self.classes_seen += 10\n",
        "      self.model.eval() # Set Network to evaluation mode\n",
        "      m=int(2000/(int(i*10+10)))\n",
        "      return self.model, batches[i]\n",
        "      \n",
        "      break\n",
        "'''\n",
        "      #NUOVO PAPER DEL PORCODDIO\n",
        "      labels_of_modified = torch.zeros(0, dtype = int).to('cuda')\n",
        "      for label in batches[i]:\n",
        "        labels = torch.LongTensor([self.diz[label]]*m).to('cuda')\n",
        "        labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\n",
        "      print('labels to be created', labels_of_modified)\n",
        "      print('len to be created', len(labels_of_modified))\n",
        "      number_of_images_created = m*10\n",
        "      net_student = resnet32(num_classes=100).to(self.device)\n",
        "      data_type = torch.float\n",
        "      inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device='cuda', dtype=data_type)\n",
        "\n",
        "      net_student = copy.deepcopy(self.model)\n",
        "      net_student.eval() #important, otherwise generated images will be non natural\n",
        "      \n",
        "      train_writer = None  # tensorboard writter\n",
        "      global_iteration = 0\n",
        "      di_lr = 0.05\n",
        "      optimizer_di = optim.Adam([inputs], lr=di_lr)\n",
        "\n",
        "      print(\"Starting model inversion\")\n",
        "      batch_idx = 0\n",
        "      inputs = get_images(net=self.model, bs=200, epochs=2000, idx=batch_idx,\n",
        "                        net_student=net_student,\n",
        "                        train_writer=train_writer, use_amp=False,\n",
        "                        optimizer=optimizer_di, inputs=inputs, \n",
        "                        var_scale=0.00005, labels=labels)\n",
        "\n",
        "\n",
        "      plt.imshow(tensor2im(inputs[0]))\n",
        "      plt.show()\n",
        "      plt.imshow(tensor2im(inputs[55]))\n",
        "      plt.show()\n",
        "      print('deepinversion finshed')\n",
        "      # update exemplars number\n",
        "      \n",
        "\n",
        "      # reduce the number of each exemplars set\n",
        "      self.reduce_old_exemplars(m) \n",
        "\n",
        "      self.cumulative_class_mean = {}\n",
        "\n",
        "      for classlabel in batches[i]:\n",
        "        indexes_class = self.original_training_set.get_class_indexes(classlabel)\n",
        "        current_class = Subset(self.original_training_set, indexes_class)\n",
        "        #self.get_new_exemplars(indexes_class, m)\n",
        "        self.get_new_exemplars(current_class, m)\n",
        "        indexes_class = self.original_training_set.get_class_indexes(classlabel)\n",
        "        current_class = Subset(self.original_training_set, indexes_class)\n",
        "\n",
        "        loader = torch.utils.data.DataLoader(current_class, batch_size=self.batch_size,shuffle=False, num_workers=4)\n",
        "        features = np.zeros((0, 64))\n",
        "        with torch.no_grad():\n",
        "          for indexes, images, labels in loader:\n",
        "            images = images.cuda()\n",
        "            feature = self.feature_extractor(images).data.cpu().numpy()\n",
        "            feature = normalize(feature, axis=1, norm='l2')\n",
        "            features = np.concatenate((features,feature), axis=0)\n",
        "\n",
        "        class_mean = np.mean(features, axis=0)\n",
        "        class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\n",
        "\n",
        "        self.cumulative_class_mean[classlabel] = class_mean\n",
        "        \n",
        "      #plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      #plt.show()\n",
        "      \n",
        "  \n",
        "      #self.exemplar_level_optimization(m, i, self.original_training_set.get_batch_indexes()[i])\n",
        "\n",
        "      # compute means of exemplar set\n",
        "      # cycle for each exemplar set\n",
        "      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device)\n",
        "      self.exemplar_labels = []\n",
        "      for j in range(len(self.exemplar_sets_idxs)):\n",
        "        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets_idxs[j])\n",
        "        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
        "        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device) # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\n",
        "      \n",
        "        with torch.no_grad():\n",
        "          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets_idxs[j][0]) \n",
        "          self.exemplar_labels.append(exemplar_label)\n",
        "          # cycle for each batch in the current exemplar set\n",
        "          for _,  exemplars, _ in exemplars_loader:\n",
        "          \n",
        "            # get exemplars features\n",
        "            exemplars = exemplars.to(self.device)\n",
        "            features = self.model.features(exemplars) # shape: (len(exemplars), feature_size)\n",
        "          \n",
        "            # normalize \n",
        "            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\n",
        "            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\n",
        "            features = features/feature_norms\n",
        "          \n",
        "            # concatenate over columns\n",
        "            ex_features = torch.cat((ex_features, features), dim=0)\n",
        "          \n",
        "        # compute current exemplar set mean and normalize it\n",
        "        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\n",
        "        ex_mean = ex_mean/torch.norm(ex_mean)\n",
        "        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\n",
        "        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar sets, feature size)\n",
        "      \n",
        "      #if i == 0:\n",
        "       # self.plot_data(self.trainloader)\n",
        "      print('accuracy on training set:', 100*self.__accuracy_fc(self.trainloader,self.diz))\n",
        "      # print('accuracy on test set:', self.__accuracy_on(self.testloader,self,self.diz))\n",
        "      current_test_acc = self.__accuracy_nme(self.testloader)\n",
        "      print('accuracy on test set:', 100*current_test_acc)\n",
        "      print('-' * 80)\n",
        "      test_acc.append(current_test_acc)\n",
        "\n",
        "    # compute comfusion matrix and save results\n",
        "    cm = self.plot_confusion_matrix()\n",
        "    with open('iCaRL_'+str(self.randomseed)+\"_cm\", 'wb') as file:\n",
        "      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open('iCaRL_'+str(self.randomseed)+\"_testacc\", 'wb') as file:\n",
        "      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "      print('PRINT IMAGES')\n",
        "      print('with data augmentation')\n",
        "      plt.imshow(self.original_training_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      plt.show()\n",
        "      print('without data augmentation')\n",
        "      plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\n",
        "      plt.show()\n",
        "'''"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n      #NUOVO PAPER DEL PORCODDIO\\n      labels_of_modified = torch.zeros(0, dtype = int).to(\\'cuda\\')\\n      for label in batches[i]:\\n        labels = torch.LongTensor([self.diz[label]]*m).to(\\'cuda\\')\\n        labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\\n      print(\\'labels to be created\\', labels_of_modified)\\n      print(\\'len to be created\\', len(labels_of_modified))\\n      number_of_images_created = m*10\\n      net_student = resnet32(num_classes=100).to(self.device)\\n      data_type = torch.float\\n      inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device=\\'cuda\\', dtype=data_type)\\n\\n      net_student = copy.deepcopy(self.model)\\n      net_student.eval() #important, otherwise generated images will be non natural\\n      \\n      train_writer = None  # tensorboard writter\\n      global_iteration = 0\\n      di_lr = 0.05\\n      optimizer_di = optim.Adam([inputs], lr=di_lr)\\n\\n      print(\"Starting model inversion\")\\n      batch_idx = 0\\n      inputs = get_images(net=self.model, bs=200, epochs=2000, idx=batch_idx,\\n                        net_student=net_student,\\n                        train_writer=train_writer, use_amp=False,\\n                        optimizer=optimizer_di, inputs=inputs, \\n                        var_scale=0.00005, labels=labels)\\n\\n\\n      plt.imshow(tensor2im(inputs[0]))\\n      plt.show()\\n      plt.imshow(tensor2im(inputs[55]))\\n      plt.show()\\n      print(\\'deepinversion finshed\\')\\n      # update exemplars number\\n      \\n\\n      # reduce the number of each exemplars set\\n      self.reduce_old_exemplars(m) \\n\\n      self.cumulative_class_mean = {}\\n\\n      for classlabel in batches[i]:\\n        indexes_class = self.original_training_set.get_class_indexes(classlabel)\\n        current_class = Subset(self.original_training_set, indexes_class)\\n        #self.get_new_exemplars(indexes_class, m)\\n        self.get_new_exemplars(current_class, m)\\n        indexes_class = self.original_training_set.get_class_indexes(classlabel)\\n        current_class = Subset(self.original_training_set, indexes_class)\\n\\n        loader = torch.utils.data.DataLoader(current_class, batch_size=self.batch_size,shuffle=False, num_workers=4)\\n        features = np.zeros((0, 64))\\n        with torch.no_grad():\\n          for indexes, images, labels in loader:\\n            images = images.cuda()\\n            feature = self.feature_extractor(images).data.cpu().numpy()\\n            feature = normalize(feature, axis=1, norm=\\'l2\\')\\n            features = np.concatenate((features,feature), axis=0)\\n\\n        class_mean = np.mean(features, axis=0)\\n        class_mean = class_mean / np.linalg.norm(class_mean)  # Normalize\\n\\n        self.cumulative_class_mean[classlabel] = class_mean\\n        \\n      #plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      #plt.show()\\n      \\n  \\n      #self.exemplar_level_optimization(m, i, self.original_training_set.get_batch_indexes()[i])\\n\\n      # compute means of exemplar set\\n      # cycle for each exemplar set\\n      self.exemplar_means = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device)\\n      self.exemplar_labels = []\\n      for j in range(len(self.exemplar_sets_idxs)):\\n        exemplars_dataset = Subset(self.original_training_set, self.exemplar_sets_idxs[j])\\n        exemplars_loader = torch.utils.data.DataLoader(exemplars_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\\n        ex_features = torch.zeros((0, self.feature_size), dtype=torch.float).to(self.device) # alla fine shape: (len(exemplar_set), feature_size) --> (m, 64)\\n      \\n        with torch.no_grad():\\n          _, _, exemplar_label = self.original_training_set.__getitem__(self.exemplar_sets_idxs[j][0]) \\n          self.exemplar_labels.append(exemplar_label)\\n          # cycle for each batch in the current exemplar set\\n          for _,  exemplars, _ in exemplars_loader:\\n          \\n            # get exemplars features\\n            exemplars = exemplars.to(self.device)\\n            features = self.model.features(exemplars) # shape: (len(exemplars), feature_size)\\n          \\n            # normalize \\n            feature_norms = torch.norm(features, p=2, dim=1) # shape: len(exemplars)\\n            feature_norms.unsqueeze_(1) # shape: (len(exemplars), 1)\\n            features = features/feature_norms\\n          \\n            # concatenate over columns\\n            ex_features = torch.cat((ex_features, features), dim=0)\\n          \\n        # compute current exemplar set mean and normalize it\\n        ex_mean = torch.mean(ex_features, dim=0) # shape: feature_size --> 64\\n        ex_mean = ex_mean/torch.norm(ex_mean)\\n        ex_mean.unsqueeze_(0) # shape: (1, feature_size) --> (1, 64)\\n        self.exemplar_means = torch.cat((self.exemplar_means, ex_mean), dim=0) # shape: (n_examplar sets, feature size)\\n      \\n      #if i == 0:\\n       # self.plot_data(self.trainloader)\\n      print(\\'accuracy on training set:\\', 100*self.__accuracy_fc(self.trainloader,self.diz))\\n      # print(\\'accuracy on test set:\\', self.__accuracy_on(self.testloader,self,self.diz))\\n      current_test_acc = self.__accuracy_nme(self.testloader)\\n      print(\\'accuracy on test set:\\', 100*current_test_acc)\\n      print(\\'-\\' * 80)\\n      test_acc.append(current_test_acc)\\n\\n    # compute comfusion matrix and save results\\n    cm = self.plot_confusion_matrix()\\n    with open(\\'iCaRL_\\'+str(self.randomseed)+\"_cm\", \\'wb\\') as file:\\n      pickle.dump(cm, file, protocol=pickle.HIGHEST_PROTOCOL)\\n    with open(\\'iCaRL_\\'+str(self.randomseed)+\"_testacc\", \\'wb\\') as file:\\n      pickle.dump(test_acc, file, protocol=pickle.HIGHEST_PROTOCOL)\\n\\n      print(\\'PRINT IMAGES\\')\\n      print(\\'with data augmentation\\')\\n      plt.imshow(self.original_training_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      plt.show()\\n      print(\\'without data augmentation\\')\\n      plt.imshow(self.original_exemplar_set.dataset.data[self.exemplar_sets_idxs[0][0]])\\n      plt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtarWXiNb9nG",
        "outputId": "b6d35793-5943-452b-857a-65fc2a00fd78"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import argparse\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "# import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import collections\n",
        "\n",
        "#from resnet_cifar import ResNet34, ResNet18\n",
        "\n",
        "try:\n",
        "    from apex.parallel import DistributedDataParallel as DDP\n",
        "    from apex import amp, optimizers\n",
        "    USE_APEX = True\n",
        "except ImportError:\n",
        "    print(\"Please install apex from https://www.github.com/nvidia/apex to run this example.\")\n",
        "    print(\"will attempt to run without it\")\n",
        "    USE_APEX = False\n",
        "\n",
        "#provide intermeiate information\n",
        "debug_output = False\n",
        "debug_output = True\n",
        "\n",
        "\n",
        "class DeepInversionFeatureHook():\n",
        "    '''\n",
        "    Implementation of the forward hook to track feature statistics and compute a loss on them.\n",
        "    Will compute mean and variance, and will use l2 as a loss\n",
        "    '''\n",
        "\n",
        "    def __init__(self, module):\n",
        "        self.hook = module.register_forward_hook(self.hook_fn)\n",
        "\n",
        "    def hook_fn(self, module, input, output):\n",
        "        # hook co compute deepinversion's feature distribution regularization\n",
        "        nch = input[0].shape[1]\n",
        "\n",
        "        mean = input[0].mean([0, 2, 3])\n",
        "        var = input[0].permute(1, 0, 2, 3).contiguous().view([nch, -1]).var(1, unbiased=False)\n",
        "\n",
        "        # forcing mean and variance to match between two distributions\n",
        "        # other ways might work better, e.g. KL divergence\n",
        "        r_feature = torch.norm(module.running_var.data.type(var.type()) - var, 2) + torch.norm(\n",
        "            module.running_mean.data.type(var.type()) - mean, 2)\n",
        "\n",
        "        self.r_feature = r_feature\n",
        "        # must have no output\n",
        "\n",
        "    def close(self):\n",
        "        self.hook.remove()\n",
        "\n",
        "def get_images(net, bs=256, epochs=1000, idx=-1, var_scale=0.00005, competitive_scale=0.01,\n",
        "               net_student=None, prefix=None, train_writer = None, global_iteration=None,\n",
        "               use_amp=False, bn_reg_scale = 0.0,\n",
        "               optimizer = None, inputs = None, labels = False, l2_coeff=0.0):\n",
        "    '''\n",
        "    Function returns inverted images from the pretrained model, parameters are tight to CIFAR dataset\n",
        "    args in:\n",
        "        net: network to be inverted\n",
        "        bs: batch size\n",
        "        epochs: total number of iterations to generate inverted images, training longer helps a lot!\n",
        "        idx: an external flag for printing purposes: only print in the first round, set as -1 to disable\n",
        "        var_scale: the scaling factor for variance loss regularization. this may vary depending on bs\n",
        "            larger - more blurred but less noise\n",
        "        net_student: model to be used for Adaptive DeepInversion\n",
        "        prefix: defines the path to store images\n",
        "        competitive_scale: coefficient for Adaptive DeepInversion\n",
        "        train_writer: tensorboardX object to store intermediate losses\n",
        "        global_iteration: indexer to be used for tensorboard\n",
        "        use_amp: boolean to indicate usage of APEX AMP for FP16 calculations - twice faster and less memory on TensorCores\n",
        "        optimizer: potimizer to be used for model inversion\n",
        "        inputs: data place holder for optimization, will be reinitialized to noise\n",
        "        bn_reg_scale: weight for r_feature_regularization\n",
        "        random_labels: sample labels from random distribution or use columns of the same class\n",
        "        l2_coeff: coefficient for L2 loss on input\n",
        "    return:\n",
        "        A tensor on GPU with shape (bs, 3, 32, 32) for CIFAR\n",
        "    '''\n",
        "\n",
        "    kl_loss = nn.KLDivLoss(reduction='batchmean').cuda()\n",
        "\n",
        "    # preventing backpropagation through student for Adaptive DeepInversion\n",
        "    net_student.eval()\n",
        "\n",
        "    best_cost = 1e6\n",
        "\n",
        "    # initialize gaussian inputs\n",
        "    inputs.data = torch.randn((bs, 3, 32, 32), requires_grad=True, device='cuda')\n",
        "    # if use_amp:\n",
        "    #     inputs.data = inputs.data.half()\n",
        "\n",
        "    # set up criteria for optimization\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer.state = collections.defaultdict(dict)  # Reset state of optimizer\n",
        "\n",
        "    # target outputs to generate\n",
        "    #if labels:\n",
        "    targets = labels\n",
        "    #else:\n",
        "     #   targets = torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9] * 25 + [0, 1, 2, 3, 4, 5]).to('cuda')\n",
        "\n",
        "    outputs=net(inputs.data)\n",
        "    _, predicted_teach = outputs.max(1)\n",
        "\n",
        "    outputs_student=net_student(inputs.data)\n",
        "    _, predicted_std = outputs_student.max(1)\n",
        "\n",
        "    if idx == 0:\n",
        "        print('Teacher correct out of {}: {}, loss at {}'.format(bs, predicted_teach.eq(targets).sum().item(), criterion(outputs, targets).item()))\n",
        "        print('Student correct out of {}: {}, loss at {}'.format(bs, predicted_std.eq(targets).sum().item(), criterion(outputs_student, targets).item()))\n",
        "\n",
        "    ## Create hooks for feature statistics catching\n",
        "    loss_r_feature_layers = []\n",
        "    for module in net.modules():\n",
        "        if isinstance(module, nn.BatchNorm2d):\n",
        "            loss_r_feature_layers.append(DeepInversionFeatureHook(module))\n",
        "\n",
        "    # setting up the range for jitter\n",
        "    lim_0, lim_1 = 2, 2\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # apply random jitter offsets\n",
        "        off1 = random.randint(-lim_0, lim_0)\n",
        "        off2 = random.randint(-lim_1, lim_1)\n",
        "        inputs_jit = torch.roll(inputs, shifts=(off1,off2), dims=(2,3))\n",
        "\n",
        "        # foward with jit images\n",
        "        optimizer.zero_grad()\n",
        "        net.zero_grad()\n",
        "        outputs = net(inputs_jit)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss_target = loss.item()\n",
        "\n",
        "        # competition loss, Adaptive DeepInvesrion\n",
        "        if competitive_scale != 0.0:\n",
        "            net_student.zero_grad()\n",
        "            outputs_student = net_student(inputs_jit)\n",
        "            T = 3.0\n",
        "\n",
        "            if 1:\n",
        "                # jensen shanon divergence:\n",
        "                # another way to force KL between negative probabilities\n",
        "                P = F.softmax(outputs_student / T, dim=1)\n",
        "                Q = F.softmax(outputs / T, dim=1)\n",
        "                M = 0.5 * (P + Q)\n",
        "\n",
        "                P = torch.clamp(P, 0.01, 0.99)\n",
        "                Q = torch.clamp(Q, 0.01, 0.99)\n",
        "                M = torch.clamp(M, 0.01, 0.99)\n",
        "                eps = 0.0\n",
        "                # loss_verifier_cig = 0.5 * kl_loss(F.log_softmax(outputs_verifier / T, dim=1), M) +  0.5 * kl_loss(F.log_softmax(outputs/T, dim=1), M)\n",
        "                loss_verifier_cig = 0.5 * kl_loss(torch.log(P + eps), M) + 0.5 * kl_loss(torch.log(Q + eps), M)\n",
        "                # JS criteria - 0 means full correlation, 1 - means completely different\n",
        "                loss_verifier_cig = 1.0 - torch.clamp(loss_verifier_cig, 0.0, 1.0)\n",
        "\n",
        "                loss = loss + competitive_scale * loss_verifier_cig\n",
        "\n",
        "        # apply total variation regularization\n",
        "        diff1 = inputs_jit[:,:,:,:-1] - inputs_jit[:,:,:,1:]\n",
        "        diff2 = inputs_jit[:,:,:-1,:] - inputs_jit[:,:,1:,:]\n",
        "        diff3 = inputs_jit[:,:,1:,:-1] - inputs_jit[:,:,:-1,1:]\n",
        "        diff4 = inputs_jit[:,:,:-1,:-1] - inputs_jit[:,:,1:,1:]\n",
        "        loss_var = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)\n",
        "        loss = loss + var_scale*loss_var\n",
        "\n",
        "        # R_feature loss\n",
        "        loss_distr = sum([mod.r_feature for mod in loss_r_feature_layers])\n",
        "        loss = loss + bn_reg_scale*loss_distr # best for noise before BN\n",
        "\n",
        "        # l2 loss\n",
        "        if 1:\n",
        "            loss = loss + l2_coeff * torch.norm(inputs_jit, 2)\n",
        "\n",
        "        if debug_output and epoch % 200==0:\n",
        "            print(f\"It {epoch}\\t Losses: total: {loss.item():3.3f},\\ttarget: {loss_target:3.3f} \\tR_feature_loss unscaled:\\t {loss_distr.item():3.3f}\")\n",
        "            #vutils.save_image(inputs.data.clone(),\n",
        "             #                 './{}/output_{}.png'.format(prefix, epoch//200),\n",
        "              #                normalize=True, scale_each=True, nrow=10)\n",
        "\n",
        "        if best_cost > loss.item():\n",
        "            best_cost = loss.item()\n",
        "            best_inputs = inputs.data\n",
        "\n",
        "        # backward pass\n",
        "        if use_amp:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    outputs=net(best_inputs)\n",
        "    _, predicted_teach = outputs.max(1)\n",
        "\n",
        "    outputs_student=net_student(best_inputs)\n",
        "    _, predicted_std = outputs_student.max(1)\n",
        "\n",
        "    if idx == 0:\n",
        "        print('Teacher correct out of {}: {}, loss at {}'.format(bs, predicted_teach.eq(targets).sum().item(), criterion(outputs, targets).item()))\n",
        "        print('Student correct out of {}: {}, loss at {}'.format(bs, predicted_std.eq(targets).sum().item(), criterion(outputs_student, targets).item()))\n",
        "\n",
        "    name_use = \"best_images\"\n",
        "    if prefix is not None:\n",
        "        name_use = prefix + name_use\n",
        "    next_batch = len(glob.glob(\"./%s/*.png\" % name_use)) // 1\n",
        "\n",
        "    #vutils.save_image(best_inputs[:20].clone(),\n",
        "     #                 './{}/output_{}.png'.format(name_use, next_batch),\n",
        "      #                normalize=True, scale_each = True, nrow=10)\n",
        "\n",
        "    #if train_writer is not None:\n",
        "     #   train_writer.add_scalar('gener_teacher_criteria', criterion(outputs, targets), global_iteration)\n",
        "      #  train_writer.add_scalar('gener_student_criteria', criterion(outputs_student, targets), global_iteration)\n",
        "\n",
        "       # train_writer.add_scalar('gener_teacher_acc', predicted_teach.eq(targets).sum().item() / bs, global_iteration)\n",
        "       # train_writer.add_scalar('gener_student_acc', predicted_std.eq(targets).sum().item() / bs, global_iteration)\n",
        "\n",
        "        #train_writer.add_scalar('gener_loss_total', loss.item(), global_iteration)\n",
        "        #train_writer.add_scalar('gener_loss_var', (var_scale*loss_var).item(), global_iteration)\n",
        "\n",
        "    net_student.train()\n",
        "\n",
        "    return best_inputs"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please install apex from https://www.github.com/nvidia/apex to run this example.\n",
            "will attempt to run without it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "cc66af5dc6354f27a94e0cf10ee4b939",
            "c25e865988494b32b57c559e0f3176c4",
            "adfc6e61cb8b4f45afeff11e6670ba20",
            "d962e73d2159493c840df67b6232dc18",
            "3dcb4216c04e469f851881831d6e9ab3",
            "aae5570542824ef09f0e3ce25dad4cdd",
            "1ed95664efcf4ea3ba069ccd54c09440",
            "797674f3e28d4574af33a380305ae417"
          ]
        },
        "id": "Ap6fvXN9cB98",
        "outputId": "95e34599-22b5-4f5f-89c2-538122b1f834"
      },
      "source": [
        "method = mnemonics(randomseed=203)\n",
        "model, batch = method.trainer()"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "current batches [11, 5, 62, 76, 27, 3, 96, 33, 78, 30]\n",
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc66af5dc6354f27a94e0cf10ee4b939",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=70.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHiRaF-FbGDD"
      },
      "source": [
        "from resnet import resnet50, resnet18\n",
        "\n",
        "trials = resnet18(pretrained = True).to('cuda')\n",
        "#trials.eval()"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "vdgNQmOTcD_V",
        "outputId": "055fbef9-c3a5-4e90-f3f4-b11c1d1e483c"
      },
      "source": [
        "# Train only FC layers -> Freeze convolutional Layers\n",
        "model.train()\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "model.fc = nn.Linear(64, 10).to('cuda')\n",
        "\n",
        "train_dataset = Subset(ilCIFAR100(10, 203, train = 'train'), ilCIFAR100(10, 203, train = 'train').get_batch_indexes()[0])\n",
        "test_dataset = Subset(ilCIFAR100(10, 203, train = 'test'), ilCIFAR100(10, 203, train = 'test').get_batch_indexes()[0])\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True, num_workers=4, drop_last=True)\n",
        "diz = ilCIFAR100(10, 203, train = 'train').get_dict()\n",
        "\n",
        "\n",
        "# Prepare Training\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9, weight_decay= 1e-5)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[49,63], gamma=0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy before finetuning', acc)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(15)):\n",
        "    \n",
        "  for _, inputs, labels in train_loader:\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    labels = torch.tensor([diz[c.item()] for c in labels])\n",
        "\n",
        "    labels=labels.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    outputs=model(inputs)\n",
        "\n",
        "    labels_encoded = F.one_hot(labels,10).float().to('cuda') #CAMBIARE ONE_HOT\n",
        "    loss = criterion(outputs, labels_encoded) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  scheduler.step()\n",
        "  #print(f'loss at epoch{epoch}', loss.item())\n",
        "\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy', acc)\n"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Prepare Training\\noptimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9, weight_decay= 1e-5)\\nscheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[49,63], gamma=0.2)\\ncriterion = nn.BCEWithLogitsLoss()\\n\\nfor  _, images, labels in test_loader:\\n  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\\n  labels = labels.to('cuda')\\n  images = images.to('cuda')\\n  outputs = model(images)\\n  _, preds = torch.max(outputs, dim=1)\\n  total += len(labels)\\n  correct += torch.sum(preds == labels).item()\\n\\nacc = correct / total\\nprint('test accuracy before finetuning', acc)\\n\\n\\n\\nfor epoch in tqdm(range(15)):\\n    \\n  for _, inputs, labels in train_loader:\\n    inputs = inputs.float().to('cuda')\\n    labels = torch.tensor([diz[c.item()] for c in labels])\\n\\n    labels=labels.to('cuda')\\n    optimizer.zero_grad()\\n    outputs=model(inputs)\\n\\n    labels_encoded = F.one_hot(labels,10).float().to('cuda') #CAMBIARE ONE_HOT\\n    loss = criterion(outputs, labels_encoded) \\n\\n    loss.backward()\\n    optimizer.step()\\n  \\n  scheduler.step()\\n  #print(f'loss at epoch{epoch}', loss.item())\\n\\ntotal = 0.0\\ncorrect = 0.0\\nfor  _, images, labels in test_loader:\\n  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\\n  labels = labels.to('cuda')\\n  images = images.to('cuda')\\n  outputs = model(images)\\n  _, preds = torch.max(outputs, dim=1)\\n  total += len(labels)\\n  correct += torch.sum(preds == labels).item()\\n\\nacc = correct / total\\nprint('test accuracy', acc)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFFN1KCJaQ2o",
        "outputId": "fad0a35c-61d6-4818-8f9f-49ddbbc44d23"
      },
      "source": [
        "!git clone https://github.com/huyvnphan/PyTorch_CIFAR10.git\n",
        "\n",
        "!pip install pytorch_lightning\n",
        "!python /content/PyTorch_CIFAR10/train.py --download_weights 1\n",
        "\n",
        "! cp -r /content/cifar10_models/state_dicts /content"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'PyTorch_CIFAR10'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 637 (delta 39), reused 50 (delta 22), pack-reused 552\u001b[K\n",
            "Receiving objects: 100% (637/637), 6.59 MiB | 20.13 MiB/s, done.\n",
            "Resolving deltas: 100% (222/222), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "175m2jwoEfBi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIsGltQmiHm4"
      },
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = True"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819,
          "referenced_widgets": [
            "9ea7c673275c4e0e814669d8171241a2",
            "f4667a3af1dc4812a6120806f8c21b35",
            "a628dfd4459449219e9bec1cd809abd3",
            "3df610f48fde44c5bcf48b14a22e1ac4",
            "fe644c3778444c55b317f56938747695",
            "4e210f0ebc4d4023816c57912ca93cf9",
            "ca0bba673b2b40b79545d002368148d9",
            "02a737261a8646fda2536c466f9a8fa3"
          ]
        },
        "id": "GgHHC5lGcOwH",
        "outputId": "063cf631-7e73-44d7-9a73-2b7c5799e03a"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/NVlabs/DeepInversion.git\n",
        "\n",
        "!pip install tensorboardX\n",
        "! cp -r /content/DeepInversion/cifar10/deepinversion_cifar10.py /content\n",
        "! cp -r /content/DeepInversion/cifar10/resnet_cifar.py /content\n",
        "'''\n",
        "#from resnet_cifar import ResNet18 # HO IMPORTATO LA CLASSE DEL PAPER E PROVATO CON LA LORO RESNET 18 (non cambia nulla)\n",
        "labels_of_modified = torch.zeros(0, dtype = int).to('cuda')\n",
        "for label in batch:\n",
        "  labels = torch.LongTensor([diz[label]]*20).to('cuda')\n",
        "  labels_of_modified = torch.cat((labels_of_modified, labels), dim=0)\n",
        "print('len to be created', len(labels_of_modified))\n",
        "number_of_images_created = 200\n",
        "\n",
        "teacher = copy.deepcopy(model)\n",
        "net_teacher = resnet32(num_classes=10).to('cuda')\n",
        "net_teacher.load_state_dict(teacher.state_dict())\n",
        "#net_student = resnet32(num_classes=10).to('cuda')\n",
        "#net_student = ResNet18().to('cuda')\n",
        "net_student = resnet18().to('cuda')\n",
        "net_teacher.eval()\n",
        "trials.eval()\n",
        "\n",
        "inputs = torch.randn((number_of_images_created, 3, 32, 32), requires_grad=True, device='cuda', dtype=torch.float)\n",
        "train_writer = None  # tensorboard writter\n",
        "global_iteration = 0\n",
        "di_lr = 0.05\n",
        "optimizer_di = optim.Adam([inputs], lr=di_lr)\n",
        "plt.imshow(tensor2im(inputs[0]))\n",
        "plt.show()\n",
        "print(\"Starting model inversion\")\n",
        "batch_idx = 0\n",
        "inputs = get_images(net=trials, bs=len(labels_of_modified), epochs=1500, idx=batch_idx, \n",
        "                  net_student=net_student, competitive_scale=10.0, l2_coeff = 3e-8, bn_reg_scale = 5.0,\n",
        "                  train_writer=train_writer, use_amp=False,\n",
        "                  optimizer=optimizer_di, inputs=inputs, \n",
        "                  var_scale=0.001, labels=labels_of_modified) #2.5e-5\n",
        "\n",
        "plt.imshow(tensor2im(inputs[0]))\n",
        "plt.show()\n",
        "print('deepinversion finshed')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len to be created 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Starting model inversion\n",
            "Teacher correct out of 200: 21, loss at 5.037278652191162\n",
            "Student correct out of 200: 21, loss at 2.4372386932373047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ea7c673275c4e0e814669d8171241a2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "It 0\t Losses: total: 38.427,\ttarget: 5.056 \tR_feature_loss unscaled:\t 4.002\n",
            "It 200\t Losses: total: 10.455,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.409\n",
            "It 400\t Losses: total: 9.729,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.302\n",
            "It 600\t Losses: total: 9.470,\ttarget: 0.000 \tR_feature_loss unscaled:\t 0.300\n",
            "It 800\t Losses: total: 9.227,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.238\n",
            "It 1000\t Losses: total: 9.184,\ttarget: 0.001 \tR_feature_loss unscaled:\t 0.240\n",
            "It 1200\t Losses: total: 9.081,\ttarget: 0.000 \tR_feature_loss unscaled:\t 0.253\n",
            "It 1400\t Losses: total: 9.228,\ttarget: 0.000 \tR_feature_loss unscaled:\t 0.275\n",
            "\n",
            "Teacher correct out of 200: 200, loss at 0.00042748849955387414\n",
            "Student correct out of 200: 1, loss at 2.747554302215576\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "deepinversion finshed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0-cwVNLSEEQ"
      },
      "source": [
        "fake_diz = {0:11, 1:5, 2:62, 3:76, 4:27, 5:3, 6:96, 7:33, 8:78, 9:30}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5wkwsPXIp7H",
        "outputId": "27825bcc-3ff1-4ba0-984e-a299776e734a"
      },
      "source": [
        "exemplar_dataset = ilCIFAR100(10, 203, train = 'exemplar')\n",
        "for i in range(len(labels_of_modified)):\n",
        "  exemplar_dataset.dataset.data[i] = tensor2im(inputs[i])\n",
        "  exemplar_dataset.dataset.targets[i] = fake_diz[labels_of_modified[i].item()]\n",
        "  \n",
        "exemplar_dataset = Subset(exemplar_dataset, np.arange(200))\n"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vw-LfHbQjl5"
      },
      "source": [
        "final_trainloader = DataLoader(torch.utils.data.ConcatDataset([train_dataset, exemplar_dataset]), batch_size=256, shuffle=True, num_workers=4, pin_memory=True)\n",
        "fake_model = resnet32(num_classes=10).to(self.device)\n",
        "\n",
        "optimizer = optim.SGD(fake_model.parameters(), lr=2, momentum=0.9, weight_decay= 1e-5)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[49,63], gamma=0.2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in tqdm(range(70)):\n",
        "    \n",
        "  for _, inputs, labels in final_trainloader:\n",
        "    inputs = inputs.float().to('cuda')\n",
        "    labels = torch.tensor([diz[c.item()] for c in labels])\n",
        "\n",
        "    labels=labels.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    outputs=model(inputs)\n",
        "\n",
        "    labels_encoded = F.one_hot(labels,10).float().to('cuda') #CAMBIARE ONE_HOT\n",
        "    loss = criterion(outputs, labels_encoded) \n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  scheduler.step()\n",
        "\n",
        "total = 0.0\n",
        "correct = 0.0\n",
        "for  _, images, labels in test_loader:\n",
        "  labels = torch.tensor([torch.tensor(diz[c.item()]) for c in labels])\n",
        "  labels = labels.to('cuda')\n",
        "  images = images.to('cuda')\n",
        "  outputs = model(images)\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  total += len(labels)\n",
        "  correct += torch.sum(preds == labels).item()\n",
        "\n",
        "acc = correct / total\n",
        "print('test accuracy with syntetic exemplars', acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}